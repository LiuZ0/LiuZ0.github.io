
https://docs.openstack.org/zh_CN/user-guide/cli.html

OpenStack commands


Ubuntu devstack install OpenStack
unstack.sh
stack.sh


预准备
root用户下执行
mkdir ~/.pip&&vi ~/.pip/pip.conf

stack用户下执行
mkdir ~/.pip&&vi ~/.pip/pip.conf

[global]
index-url = https://pypi.douban.com/simple
download_cache = ~/.cache/pip
[install]
use-mirrors = true
mirrors = http://pypi.douban.com/



设置主机名
hostnamectl set-hostname controller

添加主机映射
cat << EOF >> /etc/hosts
192.168.0.133 controller
EOF


sudo useradd -s /bin/bash -d /opt/stack -m stack

echo "stack ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/stack
sudo su - stack

git clone https://git.openstack.org/openstack-dev/devstack
cd devstack

编辑localrc.conf
[[local|localrc]]
ADMIN_PASSWORD=000000
DATABASE_PASSWORD=000000
RABBIT_PASSWORD=000000
SERVICE_PASSWORD=$ADMIN_PASSWORD


./stack.sh




RDO  install OpenStack Rocky
关闭防火墙
systemctl stop firewalld
systemctl disable firewalld

setenforce 0

sed -i 's/=enforcing/=disabled/' /etc/selinux/config

yum install yum-fastestmirror 


systemctl disable firewalld
systemctl stop firewalld
systemctl disable NetworkManager
systemctl stop NetworkManager
systemctl enable network
systemctl start network



设置主机名
hostnamectl set-hostname controller

hostnamectl set-hostname compute

 
添加主机映射
cat << EOF >> /etc/hosts
178.128.52.7 controller

EOF



官网是yum安装centos-release-openstack-rocky，配置阿里的源
cat << EOF >> /etc/yum.repos.d/openstack.repo
[openstack-rocky]
name=openstack-rocky
baseurl=https://mirrors.aliyun.com/centos/7/cloud/x86_64/openstack-rocky/
enabled=1
gpgcheck=0
[qume-kvm]
name=qemu-kvm
baseurl= https://mirrors.aliyun.com/centos/7/virt/x86_64/kvm-common/
enabled=1
gpgcheck=0
EOF


On RHEL:
$ sudo yum install -y https://www.rdoproject.org/repos/rdo-release.rpm
$ sudo yum update -y
$ sudo yum install -y openstack-packstack
$ sudo packstack --allinone



yum update -y
yum install -y centos-release-openstack-rocky
yum update -y
yum install -y openstack-packstack
packstack --allinone



CentOS7.5安装OpenStack Rocky版本
https://docs.openstack.org/install-guide/
密码的设置，都设置成000000
 
预备
关闭防火墙
systemctl restart network

systemctl stop firewalld

systemctl disable firewalld

setenforce 0

sed -i 's/=enforcing/=disabled/' /etc/selinux/config


CentOS7 Failed to start LSB: Bring up/down networking.解决方法
修改网卡mac地址 08:00:27:ef:be:a2
 

更新软件包
yum upgrade -y

更新完成后重启系统
reboot
 

设置主机名
hostnamectl set-hostname controller

hostnamectl set-hostname compute
 

添加主机映射

cat << EOF >> /etc/hosts
192.168.56.122 controller
192.168.56.123 compute
192.168.56.111 storage
EOF


cat << EOF >> /etc/hosts
178.128.133.82 controller
167.99.1.214   controller
EOF
 

配置时间同步
controller节点
安装软件包
yum install -y chrony

 

编辑/etc/chrony.conf文件
server controller iburst
allow 192.168.0.0/16

 

启动服务
systemctl start chronyd
systemctl enable chronyd

 

compute节点
安装软件包
yum install -y chrony
 

编辑/etc/chrony.conf文件
server controller iburst
 

启动服务
systemctl start chronyd
systemctl enable chronyd
 

###取消掉###
配置OpenStack-rocky的yum源文件
官网是yum安装centos-release-openstack-rocky，手动配置了阿里的源

cat << EOF >> /etc/yum.repos.d/openstack.repo
[openstack-rocky]
name=openstack-rocky
baseurl=https://mirrors.aliyun.com/centos/7/cloud/x86_64/openstack-rocky/
enabled=1
gpgcheck=0
[qume-kvm]
name=qemu-kvm
baseurl= https://mirrors.aliyun.com/centos/7/virt/x86_64/kvm-common/
enabled=1
gpgcheck=0
EOF


安装OpenStack客户端和selinux服务
yum install -y python-openstackclient openstack-selinux

安装数据库服务
在controller节点安装数据库
yum install -y mariadb mariadb-server python2-PyMySQL
 

修改数据库配置文件
新建数据库配置文件
/etc/my.cnf.d/openstack.cnf
添加以下内容

[mysqld]
bind-address = 192.168.56.122
default-storage-engine = innodb
innodb_file_per_table = on
max_connections = 4096
collation-server = utf8_general_ci
character-set-server = utf8

 

启动数据库服务
systemctl enable mariadb.service
systemctl start mariadb.service

 
设置数据库密码
运行mysql_secure_installation命令，创建数据库root密码

mysql_secure_installation

安装消息队列服务
在controller节点安装rabbitmq-server
yum install -y rabbitmq-server -y
 

启动消息队列服务
systemctl start rabbitmq-server.service
systemctl enable rabbitmq-server.service

systemctl restart rabbitmq-server.service

route add default gw 192.168.56.1

添加openstack用户
rabbitmqctl add_user openstack 000000


设置openstack用户最高权限
rabbitmqctl set_permissions openstack ".*" ".*" ".*"

 

安装memcached 服务
在controller节点上安装memcached
yum install -y memcached

 

修改memcached配置文件
编辑/etc/sysconfig/memcached，修改以下内容

修改OPTIONS="-l 127.0.0.1,::1"为

OPTIONS="-l 127.0.0.1,::1,controller"

 

启动memcached服务
systemctl start memcached.service
systemctl enable memcached.service

 

Etcd是集群中的一个十分重要的组件，用于保存集群所有的网络配置和对象的状态信息
安装etcd服务
在controller节点上安装etcd服务
yum install etcd -y

修改etcd配置文件，使其他节点能够访问
编辑/etc/etcd/etcd.conf，在各自的位置修改以下内容

#[Member]
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_PEER_URLS="http://192.168.56.120:2380"
ETCD_LISTEN_CLIENT_URLS="http://192.168.56.120:2379"
ETCD_NAME="controller"


#[Clustering]
ETCD_INITIAL_ADVERTISE_PEER_URLS="http://192.168.56.120:2380"
ETCD_ADVERTISE_CLIENT_URLS="http://192.168.56.120:2379"
ETCD_INITIAL_CLUSTER="controller=http://192.168.56.120:2380"
ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster-01"
ETCD_INITIAL_CLUSTER_STATE="new"
 


启动etcd服务
systemctl start etcd
systemctl enable etcd
 

集群健康检查
etcdctl cluster-health
 

安装keystone服务
创建数据库
mysql -uroot -p000000
 

CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost' IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%' IDENTIFIED BY '000000';
FLUSH PRIVILEGES;
 
安装软件包
yum install openstack-keystone httpd mod_wsgi -y
 

编辑配置文件
/etc/keystone/keystone.conf

[database]
connection = mysql+pymysql://keystone:000000@controller/keystone

[token]
provider = fernet
 

同步数据库
su -s /bin/sh -c "keystone-manage db_sync" keystone
 

初始化fernet key库
keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
 

引导身份认证
keystone-manage bootstrap --bootstrap-password 000000 \
--bootstrap-admin-url http://controller:5000/v3/ \
--bootstrap-internal-url http://controller:5000/v3/ \
--bootstrap-public-url http://controller:5000/v3/ \
--bootstrap-region-id RegionOne

 
编辑httpd配置文件
/etc/httpd/conf/httpd.conf
ServerName controller

 
创建文件链接
ln -s /usr/share/keystone/wsgi-keystone.conf /etc/httpd/conf.d/
 

启动httpd服务
systemctl restart httpd

systemctl enable httpd


 

编写环境变量脚本admin-openrc
export OS_USERNAME=admin
export OS_PASSWORD=000000
export OS_PROJECT_NAME=admin
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_AUTH_URL=http://controller:5000/v3
export OS_IDENTITY_API_VERSION=3
export OS_IMAGE_API_VERSION=2
 

创建service项目
openstack project create --domain default \
--description "Service Project" service


验证
openstack user list


openstack token issue



Failed to discover available identity versions when contacting http://controller:5000/v3. Attempting to parse version from URL.
是网络没有通。




//------------------------------------------------
Install glance
  
mysql -u root -p000000

CREATE DATABASE glance;

GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost' \
  IDENTIFIED BY 'GLANCE_DBPASS';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%' \
  IDENTIFIED BY 'GLANCE_DBPASS';

FLUSH PRIVILEGES;

. admin-openrc


update user set password=password("000000") where user="glance";


openstack user create --domain default --password-prompt glance


openstack role add --project service --user glance admin


openstack service create --name glance \
  --description "OpenStack Image" image


openstack endpoint create --region RegionOne \
  image public http://controller:9292


openstack endpoint create --region RegionOne \
  image internal http://controller:9292


openstack endpoint create --region RegionOne \
  image admin http://controller:9292


yum install openstack-glance
Edit the /etc/glance/glance-api.conf file and complete the following actions:


[database]
# ...
connection = mysql+pymysql://glance:000000@controller/glance

In the [keystone_authtoken] and [paste_deploy] sections, configure Identity service access:

[keystone_authtoken]
# ...
auth_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = glance
password = GLANCE_PASS

[paste_deploy]
# ...
flavor = keystone

[glance_store]
# ...
stores = file,http
default_store = file
filesystem_store_datadir = /var/lib/glance/images/


Edit the /etc/glance/glance-registry.conf file and complete the following actions:


[database]
# ...
connection = mysql+pymysql://glance:000000@controller/glance


[keystone_authtoken]
# ...
auth_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = Default
user_domain_name = Default
project_name = service
username = glance
password = 000000

[paste_deploy]
# ...
flavor = keystone


# 
su -s /bin/sh -c "glance-manage db_sync" glance


systemctl enable openstack-glance-api.service \
  openstack-glance-registry.service
systemctl start openstack-glance-api.service \
  openstack-glance-registry.service


 

 


验证
. admin-openrc

yum -y install wget
wget http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img


I had done the openstack service create --name glance.... twice So, I deleted it with "openstack service delete {id} where {id} was found via the openstack service list



openstack image create "cirros" --file cirros-0.3.4-x86_64-disk.img --disk-format qcow2 --container-format bare --public

openstack image create "cirros1" --file cirros-0.4.0-x86_64-disk.img --disk-format qcow2 --container-format bare --public



openstack image list

//---------------------------------------------------------

安装nova服务
controller节点
创建数据库
mysql -u root -p000000

drop DATABASE nova_api;
drop DATABASE nova;
drop DATABASE nova_cell0;
drop DATABASE placement;

 

CREATE DATABASE nova_api;

CREATE DATABASE nova;

CREATE DATABASE nova_cell0;

CREATE DATABASE placement;
 
GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'localhost' \
IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON nova_api.* TO 'nova'@'%' \
IDENTIFIED BY '000000';
 

GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost' \
IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%' \
IDENTIFIED BY '000000';

 
GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'localhost' \
IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON nova_cell0.* TO 'nova'@'%' \
IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'localhost' \
IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON placement.* TO 'placement'@'%' \
IDENTIFIED BY '000000';

flush privileges;
 

创建相关用户、服务
[root@controller ~]# 
openstack user create --domain default --password-prompt nova
User Password:
Repeat User Password:

[root@controller ~]# 
openstack role add --project service --user nova admin

openstack service create --name nova \
--description "OpenStack Compute" compute


openstack endpoint create --region RegionOne \
compute public http://controller:8774/v3


openstack endpoint create --region RegionOne \
compute internal http://controller:8774/v3

openstack endpoint create --region RegionOne \
compute admin http://controller:8774/v3


openstack user create --domain default --password-prompt placement
User Password:
Repeat User Password:


[root@controller ~]# 
openstack role add --project service --user placement admin

[root@controller ~]#  
openstack service create --name placement \
--description "Placement API" placement


[root@controller ~]# 
openstack endpoint create --region RegionOne \
placement public http://controller:8778


openstack endpoint create --region RegionOne \
placement internal http://controller:8778


openstack endpoint create --region RegionOne \
placement admin http://controller:8778

 

安装软件包
[root@controller ~]# 
yum install openstack-nova-api openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler openstack-nova-placement-api -y

 

 

编辑配置文件/etc/nova/nova.conf
[DEFAULT]
enabled_apis = osapi_compute,metadata
transport_url = rabbit://openstack:000000@controller

[api_database]
connection = mysql+pymysql://nova:000000@controller/nova_api

[database]
connection = mysql+pymysql://nova:000000@controller/nova
 

[placement_database]
connection = mysql+pymysql://placement:000000@controller/placement


[api]
auth_strategy = keystone

[keystone_authtoken]
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = 000000
 

[DEFAULT]
my_ip = 192.168.56.113

[DEFAULT]
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver

[vnc]
enabled = true
server_listen = $my_ip
server_proxyclient_address = $my_ip

[glance]
api_servers = http://controller:9292

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

[placement]
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller:5000/v3
username = placement
password = 000000

 

编辑/etc/httpd/conf.d/00-nova-placement-api.conf，添加以下内容

<Directory /usr/bin>
<IfVersion >= 2.4>
Require all granted
</IfVersion>
<IfVersion < 2.4>
Order allow,deny
Allow from all
</IfVersion>
</Directory>

 

重启httpd服务
[root@controller ~]# 
systemctl restart httpd
 

同步nova_api数据库
[root@controller ~]# 
su -s /bin/sh -c "nova-manage api_db sync" nova


su -s /bin/sh -c "nova-manage cell_v2 map_cell0" nova
 

su -s /bin/sh -c "nova-manage cell_v2 create_cell --name=cell1 --verbose" nova

 
su -s /bin/sh -c "nova-manage db sync" nova



验证cell0和cell1注册成功
[root@controller ~]# 
su -s /bin/sh -c "nova-manage cell_v2 list_cells" nova


启动服务
[root@controller ~]# 
systemctl restart openstack-nova-api.service \
openstack-nova-scheduler.service openstack-nova-conductor.service \
openstack-nova-novncproxy.service openstack-nova-conductor

[root@controller ~]# 
systemctl enable openstack-nova-api.service \
openstack-nova-scheduler.service openstack-nova-conductor.service \
openstack-nova-novncproxy.service openstack-nova-conductor


systemctl stop openstack-nova-api.service \
openstack-nova-scheduler.service openstack-nova-conductor.service \
openstack-nova-novncproxy.service openstack-nova-conductor


官网没有启动nova-conductor服务，这个服务是交互数据库的，如果不启动这个服务，虚拟机创建不成功

 

compute节点
安装软件包
[root@compute ~]# 
yum install openstack-nova-compute -y
 

编辑配置文件/etc/nova/nova.conf
[DEFAULT]
enabled_apis = osapi_compute,metadata

 

[DEFAULT]
transport_url = rabbit://openstack:000000@controller

 

[api]
auth_strategy = keystone
 

[keystone_authtoken]
auth_url = http://controller:5000/v3
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = nova
password = 000000

 

[DEFAULT]
my_ip = 192.168.100.20

 

[DEFAULT]
use_neutron = true
firewall_driver = nova.virt.firewall.NoopFirewallDriver

 

[vnc]
enabled = true
server_listen = 0.0.0.0
server_proxyclient_address = $my_ip
novncproxy_base_url = http:// 192.168.100.10:6080/vnc_auto.html

 

[glance]
api_servers = http://controller:9292

 

[oslo_concurrency]
lock_path = /var/lib/nova/tmp

 

[placement]
region_name = RegionOne
project_domain_name = Default
project_name = service
auth_type = password
user_domain_name = Default
auth_url = http://controller:5000/v3
username = placement
password = 000000

 

检查是否支持虚拟化
egrep -c '(vmx|svm)' /proc/cpuinfo

如果等于0，则要在/etc/nova/nova.conf的[libvirt]下添加以下参数

[libvirt]
virt_type = qemu



systemctl restart openstack-nova-compute.service
 

启动服务
systemctl start libvirtd.service openstack-nova-compute.service

systemctl enable libvirtd.service openstack-nova-compute.service


controller节点
确认数据库中有计算节点
# 
. admin-openrc

[root@controller ~]# 
openstack compute service list --service nova-compute

 

发现计算节点
[root@controller ~]# 
su -s /bin/sh -c "nova-manage cell_v2 discover_hosts --verbose" nova

启动服务并设置开机启动

systemctl enable libvirtd.service openstack-nova-compute.service
systemctl start libvirtd.service openstack-nova-compute.service
验证

openstack compute service list --service nova-compute

 

如果想要自动发现新compute节点，可以在/etc/nova/nova.conf的[scheduler]下添加以下参数

[scheduler]
discover_hosts_in_cells_interval = 300


 
安装neutron服务
controller节点
创建数据库
[root@controller ~]# 
mysql -uroot -p000000

drop database neutron;
 
create database neutron;

MariaDB [(none)]> 
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost' \
IDENTIFIED BY '000000';

GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'%' \
IDENTIFIED BY '000000';

FLUSH PRIVILEGES;

创建用户、服务
[root@controller ~]# 
openstack user create --domain default --password-prompt neutron

User Password:
Repeat User Password:


[root@controller ~]# 
openstack role add --project service --user neutron admin

[root@controller ~]# 
openstack service create --name neutron \
--description "OpenStack Networking" network

openstack endpoint create --region RegionOne \
network public http://controller:9696


openstack endpoint create --region RegionOne \
network internal http://controller:9696


openstack endpoint create --region RegionOne \
network admin http://controller:9696


配置provider network网络
安装软件包
[root@controller ~]# 
yum install openstack-neutron openstack-neutron-ml2  openstack-neutron-linuxbridge ebtables -y

 

编辑/etc/neutron/neutron.conf配置文件
[database]
connection = mysql+pymysql://neutron:000000@controller/neutron


[DEFAULT]
core_plugin = ml2
service_plugins =

 

[DEFAULT]
transport_url = rabbit://openstack:000000@controller

 
[DEFAULT]
auth_strategy = keystone
 

[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = 000000
 

[DEFAULT]
notify_nova_on_port_status_changes = true
notify_nova_on_port_data_changes = true

 

[nova]
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = 000000
 

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp

 

编辑配置文件/etc/neutron/plugins/ml2/ml2_conf.ini
[ml2]
type_drivers = flat,vlan

[ml2]
tenant_network_types =

 

[ml2]
mechanism_drivers = linuxbridge
 

[ml2]
extension_drivers = port_security
 

[ml2_type_flat]
flat_networks = provider
 

[securitygroup]
enable_ipset = true

 

编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini配置文件
[linux_bridge]
physical_interface_mappings = provider:eth1
 

[vxlan]
enable_vxlan = false

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver

 

编辑配置文件/etc/neutron/dhcp_agent.ini
[DEFAULT]
interface_driver = linuxbridge
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true
 

配置Self-service网络
安装软件包
# 
yum install openstack-neutron openstack-neutron-ml2 openstack-neutron-linuxbridge ebtables -y

配置/etc/neutron/neutron.conf文件
[database]
connection = mysql+pymysql://neutron:000000@controller/neutron
 

[DEFAULT]
core_plugin = ml2
service_plugins = router
allow_overlapping_ips = true
 

[DEFAULT]
transport_url = rabbit://openstack:000000@controller
 

[DEFAULT]
auth_strategy = keystone
 

[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = 000000

 

[DEFAULT]
notify_nova_on_port_status_changes = true
notify_nova_on_port_data_changes = true
 

[nova]
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = nova
password = 000000
 

[oslo_concurrency]
lock_path = /var/lib/neutron/tmp


编辑/etc/neutron/plugins/ml2/ml2_conf.ini文件
[ml2]
type_drivers = flat,vlan,vxlan
 

[ml2]
tenant_network_types = vxlan
 

[ml2]
mechanism_drivers = linuxbridge,l2population
 

[ml2]
extension_drivers = port_security
 

[ml2_type_flat]
flat_networks = provider

[ml2_type_vxlan]
vni_ranges = 1:1000

[securitygroup]
enable_ipset = true
 

编辑/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件
[linux_bridge]
physical_interface_mappings = provider:eth1

[vxlan]
enable_vxlan = true
local_ip = 192.168.200.10
l2_population = true
 

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
 

编辑/etc/neutron/l3_agent.ini文件
[DEFAULT]
interface_driver = linuxbridge
 

编辑/etc/neutron/dhcp_agent.ini文件
[DEFAULT]
interface_driver = linuxbridge
dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
enable_isolated_metadata = true
 

编辑/etc/neutron/metadata_agent.ini文件
[DEFAULT]
nova_metadata_host = controller
metadata_proxy_shared_secret = METADATA_SECRET
 

编辑/etc/nova/nova.conf文件
[neutron]
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = 000000
service_metadata_proxy = true
metadata_proxy_shared_secret = METADATA_SECRET

 

创建链接
[root@controller ~]# 
ln -s /etc/neutron/plugins/ml2/ml2_conf.ini /etc/neutron/plugin.ini
 

同步数据库
[root@controller ~]# 
su -s /bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head" neutron 

 

启动服务
[root@controller ~]# 
systemctl restart openstack-nova-api

[root@controller ~]# 
systemctl start neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service

[root@controller ~]# 
systemctl enable neutron-server.service neutron-linuxbridge-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service

Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-server.service to /usr/lib/systemd/system/neutron-server.service.

Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-linuxbridge-agent.service to /usr/lib/systemd/system/neutron-linuxbridge-agent.service.

Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-dhcp-agent.service to /usr/lib/systemd/system/neutron-dhcp-agent.service.

Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-metadata-agent.service to /usr/lib/systemd/system/neutron-metadata-agent.service.

如果选择了Self-service网络，还需要启动这个服务

[root@controller ~]# 
systemctl start neutron-l3-agent.service

[root@controller ~]# 
systemctl enable neutron-l3-agent.service

Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-l3-agent.service to /usr/lib/systemd/system/neutron-l3-agent.service.

 

compute节点
安装软件包
[root@compute ~]# 
yum install openstack-neutron-linuxbridge ebtables ipset -y
 

编辑配置/etc/neutron/neutron.conf文件
[DEFAULT]
transport_url = rabbit://openstack:000000@controller
 

[DEFAULT]
auth_strategy = keystone
 

[keystone_authtoken]
www_authenticate_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = neutron
password = 000000

 
[oslo_concurrency]
lock_path = /var/lib/neutron/tmp


配置provider网络
编辑配置/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件
[linux_bridge]
physical_interface_mappings = provider:eth1
 

[vxlan]
enable_vxlan = false
 

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
 

配置Self-service网络
编辑配置/etc/neutron/plugins/ml2/linuxbridge_agent.ini文件
[linux_bridge]
physical_interface_mappings = provider:enp0s8
 

[vxlan]
enable_vxlan = true
local_ip = 192.168.56.112
l2_population = true
 

[securitygroup]
enable_security_group = true
firewall_driver = neutron.agent.linux.iptables_firewall.IptablesFirewallDriver
 

配置nova配置/etc/nova/nova.conf文件
[neutron]
url = http://controller:9696
auth_url = http://controller:5000
auth_type = password
project_domain_name = default
user_domain_name = default
region_name = RegionOne
project_name = service
username = neutron
password = 000000

 

启动服务
[root@compute ~]# 
systemctl restart openstack-nova-compute

[root@compute ~]# 
systemctl restart neutron-linuxbridge-agent.service

[root@compute ~]# 
systemctl enable neutron-linuxbridge-agent.service

Created symlink from /etc/systemd/system/multi-user.target.wants/neutron-linuxbridge-agent.service to /usr/lib/systemd/system/neutron-linuxbridge-agent.service.

 

验证
[root@controller ~]# 
openstack network agent list

 

安装dashboard
controller节点
安装软件包
[root@controller ~]
yum install -y openstack-dashboard
 

编辑配置文件/etc/openstack-dashboard/local_settings
OPENSTACK_HOST = "controller"

ALLOWED_HOSTS = ['*', 'localhost']

SESSION_ENGINE = 'django.contrib.sessions.backends.cache'
 

CACHES = {
'default': {
'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',
'LOCATION': 'controller:11211',

}
}

OPENSTACK_KEYSTONE_URL = "http://%s:5000/v3" % OPENSTACK_HOST

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True

OPENSTACK_API_VERSIONS = {
"identity": 3,
"image": 2,
"volume": 2,
}

OPENSTACK_KEYSTONE_DEFAULT_DOMAIN = "Default"

OPENSTACK_KEYSTONE_DEFAULT_ROLE = "user"

OPENSTACK_NEUTRON_NETWORK = {

...

'enable_router': False,

'enable_quotas': False,

'enable_distributed_router': False,

'enable_ha_router': False,

'enable_lb': False,

'enable_firewall': False,

'enable_vpn': False,

'enable_fip_topology_check': False,

}
 

编辑/etc/httpd/conf.d/openstack-dashboard.conf
WSGIApplicationGroup %{GLOBAL}

 
启动服务
[root@controller ~]# 
systemctl restart httpd.service memcached.service
 

验证
浏览器打开
192.168.56.113/dashboard


创建虚拟机
创建provider网络
[root@controller ~]# 
. admin-openrc

[root@controller ~]# 
openstack network create --share --external --provider-physical-network provider --provider-network-type flat provider


创建子网
[root@controller ~]# 
openstack subnet create --network provider --allocation-pool start=192.168.56.150,end=192.168.56.200 --dns-nameserver 8.8.8.8 --gateway 192.168.56.1 --subnet-range 192.168.56.0/24 provider

 

创建Self-service网络
[root@controller ~]# 
openstack network create selfservice

[root@controller ~]# 
openstack subnet create --network selfservice --dns-nameserver 8.8.4.4 --gateway 172.16.1.1 --subnet-range 172.16.1.0/24 selfservice


创建路由
openstack router create router
 

创建子网接口
openstack router add subnet router selfservice
 

创建网关    
openstack router set router --external-gateway provider
 

创建类型
openstack flavor create --id 0 --vcpus 1 --ram 64 --disk 1 mm1.tiny
 

创建一个Self-service网络的虚拟机
这里的net-id是
openstack network list
查看到的id
[root@controller ~]# 
openstack server create --flavor mm1.tiny --image cirros --nic net-id=4fb57fba-3288-4cc5-a62a-cb2990a24358 cirros

openstack server create --flavor m1.tiny --image cirros-0.4.0-x86_64-disk --nic net-id=0b83fe27-b978-40b1-88b0-1661a1f39563 vm1


openstack server create --flavor m1.tiny --image cirros-0.4.0-x86_64-disk --nic net-id=0b83fe27-b978-40b1-88b0-1661a1f3956 vm5




openstack server create --flavor m1.tiny --image cirros --nic net-id=public vm1

openstack server create --flavor m1.tiny --image cirros-0.4.0-x86_64-disk --nic net-id=069dec3c-8322-4ae0-a250-ffbd49aebe07 cmd-created-vm

查看是否创建成功
[root@controller ~]# 
openstack server list


m1.tiny
openstack server create --flavor m1.tiny --image cirros --nic net-id=27109cd0-1fee-4931-98ae-c1e35cbe0ed7 cirros


//-------------------------------------------------------------------
八. 安装和配置swift
《一》安装和配置控制器节点

<一>前提条件 
代理服务依赖于身份验证和授权机制，如身份服务。但是，与其他服务不同的是，它还提供了一种内部机制，允许其在没有任何其他OpenStack服务的情况下运行。在配置对象存储服务之前，您必须创建服务凭证和API端点。 
1.来源admin凭据来访问仅管理员CLI命令：

# . admin-openrc

2.要创建身份服务凭据，请完成以下步骤： 
A.创建swift用户：
openstack user create --domain default --password-prompt swift

B.将admin角色添加到swift用户：
openstack role add --project service --user swift admin

C.创建swift服务实体：
openstack service create --name swift \
  --description "OpenStack Object Storage" object-store

3.创建对象存储服务API端点：
openstack endpoint create --region RegionOne \
  object-store public http://controller:8080/v1/AUTH_%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  object-store internal http://controller:8080/v1/AUTH_%\(tenant_id\)s
openstack endpoint create --region RegionOne \
  object-store admin http://controller:8080/v1


<二>安装和配置的部件

1、安装软件包：

yum install -y openstack-swift-proxy python-swiftclient \
  python-keystoneclient python-keystonemiddleware \
  memcached

2、从Object Storage源存储库获取代理服务配置文件：
curl -o /etc/swift/proxy-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/proxy-server.conf-sample?h=stable/rocky

3、 编辑/etc/swift/proxy-server.conf文件并完成以下操作：

A.在该[DEFAULT]部分中，配置绑定端口，用户和配置目录：

[DEFAULT]
...
bind_port = 8080
user = swift
swift_dir = /etc/swift

B.在[pipeline:main]部分中，
****删除tempurl和 tempauth模块并添加authtoken和keystoneauth 模块：

[pipeline:main]
pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server

pipeline = catch_errors gatekeeper healthcheck proxy-logging cache container_sync bulk ratelimit authtoken keystoneauth container-quotas account-quotas slo dlo versioned_writes proxy-logging proxy-server


C.在[app:proxy-server]部分中，启用自动帐户创建：

[app:proxy-server]
use = egg:swift#proxy
...
account_autocreate = True


[filter:keystoneauth]
use = egg:swift#keystoneauth
operator_roles = admin,user


D.在[filter:keystoneauth]部分中，配置操作员角色：

[filter:keystoneauth]
use = egg:swift#keystoneauth
...
operator_roles = admin,user




E.在[filter:authtoken]小节中，配置身份服务访问：

[filter:authtoken]
paste.filter_factory = keystonemiddleware.auth_token:filter_factory
...
auth_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = swift
password = 000000
delay_auth_decision = True


F.在[filter:cache]部分中，配置memcached位置：

[filter:cache]
use = egg:swift#memcache
...
memcache_servers = controller:11211


安装和配置存储节点
（此处的存储节点为计算节点和网络节点，分别在两个节点都运行以下步骤，以下是以其中一个节点为例）

<一>先决条件
在安装和配置存储节点上的对象存储服务之前，必须准备存储设备（添加两块硬盘，大小为5G）

1、 安装支持实用程序包： 
yum install -y xfsprogs rsync

2、 格式/dev/sdb和/dev/sdc设备XFS：

mkfs.xfs /dev/sdb
mkfs.xfs /dev/sdc


3、 创建挂载点目录结构：

mkdir -p /srv/node/sdb
mkdir -p /srv/node/sdc


4、 编辑/etc/fstab文件，添加以下:

/dev/sdb /srv/node/sdb xfs noatime,nodiratime,nobarrier,logbufs=8 0 2
/dev/sdc /srv/node/sdc xfs noatime,nodiratime,nobarrier,logbufs=8 0 2


5、 安装设备：

mount /srv/node/sdb
mount /srv/node/sdc


6、 创建或编辑/etc/rsyncd.conf文件包含以下：

uid = swift
gid = swift
log file = /var/log/rsyncd.log
pid file = /var/run/rsyncd.pid
address =192.168.56.122
[account]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/account.lock
[container]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/container.lock
[object]
max connections = 2
path = /srv/node/
read only = False
lock file = /var/lock/object.lock



7、开始rsyncd服务并将其配置为在系统启动时启动：
systemctl enable rsyncd.service
systemctl start rsyncd.service    


/usr/bin/rsync --daemon --config=/etc/rsyncd.conf

<二>安装和配置组件
1.安装软件包;

yum install -y openstack-swift-account openstack-swift-container \
  openstack-swift-object

2.从对象存储源存储库获取accounting, container, and object服务配置文件：

curl -o /etc/swift/account-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/account-server.conf-sample?h=stable/rocky
curl -o /etc/swift/container-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/container-server.conf-sample?h=stable/rocky
curl -o /etc/swift/object-server.conf https://git.openstack.org/cgit/openstack/swift/plain/etc/object-server.conf-sample?h=stable/rocky



3、 编辑/etc/swift/account-server.conf文件并完成以下操作：

A.在[DEFAULT]部分中，配置绑定IP地址、绑定端口、用户、配置目录和挂载点目录：

[DEFAULT]
...
bind_ip = 192.168.56.123
bind_port = 6202
user = swift
swift_dir = /etc/swift
devices = /srv/node
mount_check = True


B. 在[pipeline:main]部分，启用适当的模块：

[pipeline:main]
pipeline = healthcheck recon account-server


C. 在[filter:recon]部分，配置 recon (meters)缓存目录：

[filter:recon]
use = egg:swift#recon
...
recon_cache_path = /var/cache/swift




4、 编辑/etc/swift/container-server.conf文件并完成以下操作：

A.在[DEFAULT]部分中，配置绑定IP地址、绑定端口、用户、配置目录和挂载点目录：

[DEFAULT]
...
bind_ip = 192.168.56.123
bind_port = 6201
user = swift
swift_dir = /etc/swift
devices = /srv/node
mount_check = True



B. 在[pipeline:main]部分，启用适当的模块：

[pipeline:main]
pipeline = healthcheck recon container-server


C. 在[filter:recon]部分，配置 recon (meters)缓存目录：

[filter:recon]
use = egg:swift#recon
...
recon_cache_path = /var/cache/swift




5、编辑/etc/swift/object-server.conf文件并完成以下操作：

A.在[DEFAULT]部分中，配置绑定IP地址、绑定端口、用户、配置目录和挂载点目录：

[DEFAULT]
...
bind_ip = 192.168.56.123
bind_port = 6200
user = swift
swift_dir = /etc/swift
devices = /srv/node
mount_check = True


B. 在[pipeline:main]部分，启用适当的模块：

[pipeline:main]
pipeline = healthcheck recon object-server


C. 在[filter:recon]部分，配置 recon (meters)缓存目录：

[filter:recon]
use = egg:swift#recon
...
recon_cache_path = /var/cache/swift
recon_lock_path = /var/lock 





6、确保挂载点目录结构的正确所有权：

chown -R swift:swift /srv/node

7、创建recon目录并确保它的正确所有权：

mkdir -p /var/cache/swift
chown -R swift:swift /var/cache/swift



《三》创建并分发初始铃声
在启动对象存储服务之前，您必须创建初始帐户，容器和对象环。环形构建器创建每个节点用来确定和部署存储体系结构的配置文件。

<一>创建账户ring
帐户服务器使用帐户环来维护容器列表。

1.转到/etc/swift目录。

2.创建基本account.builder文件：

NWR
  N = 副本数 
  W = 一次成功的写操作必须完成的写副本数 
  R = 一次成功的读操作需要读的副本数

swift-ring-builder account.builder create 10 2 1

3.将每个存储节点添加到环中：

swift-ring-builder account.builder add \
  --region 1 --zone 1 --ip 192.168.56.122 --port 6202 --device sdb --weight 100
swift-ring-builder account.builder add \
  --region 1 --zone 1 --ip 192.168.56.122 --port 6202 --device sdc --weight 100
swift-ring-builder account.builder add \
  --region 1 --zone 1 --ip 192.168.56.123 --port 6202 --device sdb --weight 100
swift-ring-builder account.builder add \
  --region 1 --zone 1 --ip 192.168.56.123 --port 6202 --device sdc --weight 100

4.验证 ring 的内容：
swift-ring-builder account.builder

5.平衡 ring：
swift-ring-builder account.builder rebalance

<二>创建容器ring
帐户服务器使用帐户 ring 来维护一个容器的列表。 
1、切换到 /etc/swift目录。

2、创建基本container.builder文件：

swift-ring-builder container.builder create 10 2 1

3、添加每个节点到 ring 中：

swift-ring-builder container.builder add \
  --region 1 --zone 1 --ip 192.168.56.122 --port 6201 --device sdb --weight 100
swift-ring-builder container.builder add \
  --region 1 --zone 1 --ip 192.168.56.122 --port 6201 --device sdc --weight 100


swift-ring-builder container.builder add \
  --region 1 --zone 1 --ip 192.168.56.123 --port 6201 --device sdb --weight 100
swift-ring-builder container.builder add \
  --region 1 --zone 1 --ip 192.168.56.123 --port 6201 --device sdc --weight 100



4、验证 ring 的内容：
swift-ring-builder container.builder

5、平衡 ring：
swift-ring-builder container.builder rebalance

<三>创建对象ring
对象服务器使用对象环来维护对象在本地设备上的位置列表。 
1、切换到 /etc/swift目录。

2、创建基本object.builder文件：
swift-ring-builder object.builder create 10 2 1

3、添加每个节点到 ring 中：
swift-ring-builder object.builder add \
  --region 1 --zone 1 --ip 192.168.56.122 --port 6200 --device sdb --weight 100 
swift-ring-builder object.builder add \
  --region 1 --zone 1 --ip 192.168.56.122 --port 6200 --device sdc --weight 100


swift-ring-builder object.builder add \
  --region 1 --zone 1 --ip 192.168.56.123 --port 6200 --device sdb --weight 100
swift-ring-builder object.builder add \
  --region 1 --zone 1 --ip 192.168.56.123 --port 6200 --device sdc --weight 100



4、验证 ring 的内容：
swift-ring-builder object.builder

5、平衡 ring：
swift-ring-builder object.builder rebalance

分发环配置文件

复制account.ring.gz，container.ring.gz和object.ring.gz 文件到每个存储节点和其他运行了代理服务的额外节点的 /etc/swift 目录。


《四》完成安装
1、/etc/swift/swift.conf从Object Storage源存储库中获取文件：

curl -o /etc/swift/swift.conf \
  https://git.openstack.org/cgit/openstack/swift/plain/etc/swift.conf-sample?h=stable/rocky


2、编辑/etc/swift/swift.conf文件并完成以下操作：

A.在该[swift-hash]部分中，为您的环境配置散列路径前缀和后缀。

[swift-hash]
...
swift_hash_path_suffix = hsystsy.cy
swift_hash_path_prefix = hsystsy.cy




B.在该[storage-policy:0]部分中，配置默认存储策略：

[storage-policy:0]
...
name = Policy-0
default = yes


3、将swift.conf文件复制到/etc/swift每个存储节点上的目录以及任何运行代理服务的其他节点。 
4、在所有节点上，确保配置目录的正确所有权：
chown -R root:swift /etc/swift

5.在控制器节点和运行代理服务的任何其他节点上，启动Object Storage代理服务（包括其依赖关系），并将其配置为在系统引导时启动：

systemctl enable openstack-swift-proxy.service memcached.service
systemctl restart openstack-swift-proxy.service memcached.service


在存储节点上，启动对象存储服务，并配置它们在系统启动时启动：
systemctl enable openstack-swift-account.service openstack-swift-account-auditor.service \
  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
systemctl start openstack-swift-account.service openstack-swift-account-auditor.service \
  openstack-swift-account-reaper.service openstack-swift-account-replicator.service
systemctl enable openstack-swift-container.service \
  openstack-swift-container-auditor.service openstack-swift-container-replicator.service \
  openstack-swift-container-updater.service
systemctl start openstack-swift-container.service \
  openstack-swift-container-auditor.service openstack-swift-container-replicator.service \
  openstack-swift-container-updater.service
systemctl enable openstack-swift-object.service openstack-swift-object-auditor.service \
  openstack-swift-object-replicator.service openstack-swift-object-updater.service
systemctl start openstack-swift-object.service openstack-swift-object-auditor.service \
  openstack-swift-object-replicator.service openstack-swift-object-updater.service 


《五》验证操作
验证对象存储服务的操作。 
如果您使用的是红帽企业版Linux 7或CentOS 7，并且其中一个或多个步骤不起作用，请检查/var/log/audit/audit.log SELinux消息文件，指出拒绝swift进程的操作。如果存在，则将/srv/node目录的安全上下文更改为swift_data_t类型，object_r 角色和system_u用户的最低安全级别（s0）：

chcon -R system_u:object_r:swift_data_t:s0 /srv/node

1、来源demo证书：
. demo-openrc

2、显示服务状态：
swift stat

3、创建container1容器：
openstack container create cont1

4、将测试文件上传到container1容器：
openstack object create cont2 testfile 

5、在container1容器中列出文件：
openstack object list container2

6、从container1容器中下载一个测试文件：
openstack object save container2 testfile




//-------------------------------------------------------------
CentOS7安装OpenStack(Rocky版)-09.安装Cinder存储服务组件（控制节点）
---Cinder install

9.0.Cinder概述
9.1.在控制节点安装cinder存储服务
1）创建cinder数据库
2）在keystone上面注册cinder服务（创建服务证书）
3）安装cinder相关软件包
4）快速修改cinder配置
5）同步cinder数据库
6）修改nova配置文件
7）重启nova-api服务
8）启动cinder存储服务
9.2.在存储节点服务器安装cinder存储服务
1）安装LVM相关软件包
2）启动LVM的metadata服务并配置开机自启动
3）创建LVM逻辑卷
4）配置过滤器，防止系统出错
5）在存储节点安装配置cinder组件
6）在存储节点快速修改cinder配置
7）在存储节点启动cinder服务并配置开机自启动
9.3.在控制节点进行验证
1）获取管理员变量
2）查看存储卷列表
 

Openstack的Cinder存储服务组件，cinder服务可以提供云磁盘（卷），类似阿里云云盘


# openstack-Mitaka-cinder块存储服务中文文档
# https://docs.openstack.org/mitaka/zh_CN/install-guide-rdo/cinder.html

# openstack-rocky版本Cinder官方安装文档
# https://docs.openstack.org/cinder/rocky/install/

9.0.Cinder概述
OpenStack块存储服务(cinder)为虚拟机添加持久的存储，块存储提供一个基础设施为了管理卷，以及和OpenStack计算服务交互，为实例提供卷。此服务也会激活管理卷的快照和卷类型的功能。

块存储服务通常包含下列组件：

1）cinder-api
接受API请求，并将其路由到``cinder-volume``执行。
2）cinder-volume
与块存储服务和例如``cinder-scheduler``的进程进行直接交互。它也可以与这些进程通过一个消息队列进行交互。``cinder-volume``服务响应送到块存储服务的读写请求来维持状态。它也可以和多种存储提供者在驱动架构下进行交互。
3）cinder-scheduler守护进程
选择最优存储提供节点来创建卷。其与``nova-scheduler``组件类似。
4）cinder-backup守护进程
``cinder-backup``服务提供任何种类备份卷到一个备份存储提供者。就像``cinder-volume``服务，它与多种存储提供者在驱动架构下进行交互。
5）消息队列
在块存储的进程之间路由信息。

9.1.在控制节点安装cinder存储服务
# Install and configure controller node
https://docs.openstack.org/cinder/rocky/install/cinder-controller-install-rdo.html

1）创建cinder数据库
# 创建相关数据库，授权访问用户

mysql -u root -p000000
----------------------------------------
CREATE DATABASE cinder;
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'localhost' IDENTIFIED BY 'cinder';
GRANT ALL PRIVILEGES ON cinder.* TO 'cinder'@'%' IDENTIFIED BY 'cinder';
flush privileges;
show databases;
select user,host from mysql.user;
exit
----------------------------------------
2）在keystone上面注册cinder服务（创建服务证书）
# 在keystone上创建cinder用户

openstack user create --domain default --password=cinder cinder
openstack user list

# 在keystone上将cinder用户配置为admin角色并添加进service项目，以下命令无输出
openstack role add --project service --user cinder admin


# 创建cinder服务的实体
openstack service create --name cinderv2 --description "OpenStack Block Storage" volumev2
openstack service create --name cinderv3 --description "OpenStack Block Storage" volumev3
openstack service list

# 创建cinder服务的API端点（endpoint）
openstack endpoint create --region RegionOne volumev2 public http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne volumev2 internal http://controller:8776/v2/%\(project_id\)s
openstack endpoint create --region RegionOne volumev2 admin http://controller:8776/v2/%\(project_id\)s

openstack endpoint create --region RegionOne volumev3 public http://controller:8776/v3/%\(project_id\)s
openstack endpoint create --region RegionOne volumev3 internal http://controller:8776/v3/%\(project_id\)s
openstack endpoint create --region RegionOne volumev3 admin http://controller:8776/v3/%\(project_id\)s
openstack endpoint list



3）安装cinder相关软件包
yum install openstack-cinder -y
yum install openstack-utils
4）快速修改cinder配置
openstack-config --set  /etc/cinder/cinder.conf database connection  mysql+pymysql://cinder:cinder@controller/cinder
openstack-config --set  /etc/cinder/cinder.conf DEFAULT transport_url rabbit://openstack:000000@controller
openstack-config --set  /etc/cinder/cinder.conf DEFAULT auth_strategy  keystone 
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken auth_uri  http://controller:5000
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken auth_url  http://controller:5000
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken memcached_servers  controller:11211
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken auth_type  password
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken project_domain_name  default 
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken user_domain_name  default
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken project_name  service 
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken username  cinder
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken password  cinder
openstack-config --set  /etc/cinder/cinder.conf DEFAULT my_ip 192.168.56.122
openstack-config --set  /etc/cinder/cinder.conf oslo_concurrency lock_path  /var/lib/nova/tmp 


5）同步cinder数据库
# 有35张表
su -s /bin/sh -c "cinder-manage db sync" cinder

# 验证数据库
mysql -h192.168.56.122 -ucinder -pcinder -e "use cinder;show tables;"

6）修改nova配置文件
# 配置nova调用cinder服务
openstack-config --set  /etc/nova/nova.conf cinder os_region_name  RegionOne

# 检查生效的nova配置
grep '^[a-z]' /etc/nova/nova.conf |grep os_region_name

7）重启nova-api服务
systemctl restart openstack-nova-api.service
systemctl status openstack-nova-api.service

8）启动cinder存储服务
# 需要启动2个服务
systemctl start openstack-cinder-api.service openstack-cinder-scheduler.service
systemctl status openstack-cinder-api.service openstack-cinder-scheduler.service

systemctl enable openstack-cinder-api.service openstack-cinder-scheduler.service
systemctl list-unit-files |grep openstack-cinder |grep enabled
# 至此，控制端的cinder服务安装完毕，在dashboard上面可以看到项目目录中多了一个卷服务


# 接下来安装块存储节点服务器storage node
9.2.在存储节点服务器安装cinder存储服务
# 存储节点建议单独部署服务器（最好是物理机），测试时也可以部署在控制节点或者计算节点
# 在本文，存储节点使用LVM逻辑卷提供服务，需要提供一块空的磁盘用以创建LVM逻辑卷
# 我这里在VMware虚拟机增加一块100GB的磁盘

1）安装LVM相关软件包
yum install lvm2 device-mapper-persistent-data -y
2）启动LVM的metadata服务并配置开机自启动
systemctl start lvm2-lvmetad.service
systemctl status lvm2-lvmetad.service

systemctl enable lvm2-lvmetad.service
systemctl list-unit-files |grep lvm2-lvmetad |grep enabled


3）创建LVM逻辑卷
# 检查磁盘状态
fdisk -l
# 创建LVM 物理卷 /dev/sdd
pvcreate /dev/sdd

# 创建 LVM 卷组 cinder-volumes，块存储服务会在这个卷组中创建逻辑卷
vgcreate cinder-volumes /dev/sdd


4）配置过滤器，防止系统出错
# 默认只会有openstack实例访问块存储卷组，不过，底层的操作系统也会管理这些设备并尝试将逻辑卷与系统关联。
# 默认情况下LVM卷扫描工具会扫描整个/dev目录，查找所有包含lvm卷的块存储设备。如果其他项目在某个磁盘设备sda，sdc等上使用了lvm卷，那么扫描工具检测到这些卷时会尝试缓存这些lvm卷，可能导致底层操作系统或者其他服务无法正常调用他们的lvm卷组，从而产生各种问题，需要手动配置LVM，让LVM卷扫描工具只扫描包含"cinder-volume"卷组的设备/dev/sdb，我这边磁盘分区都是格式化的手工分区，目前不存在这个问题，以下是配置演示

vim /etc/lvm/lvm.conf
-----------------------------
devices {
filter = [ "a/sdb/", "r/.*/"]
}
-----------------------------

# 配置规则：
# 每个过滤器组中的元素都以a开头accept接受，或以 r 开头reject拒绝，后面连接设备名称的正则表达式规则。
# 过滤器组必须以"r/.*/"结束，过滤所有保留设备。
# 可以使用命令:vgs -vvvv来测试过滤器。
# 注意：

# 如果存储节点的操作系统磁盘/dev/sda使用的是LVM卷组，也需要将该设备添加到过滤器中，例如：
filter = [ "a/sda/", "a/sdb/", "r/.*/"]
# 如果计算节点的操作系统磁盘/dev/sda使用的是LVM卷组，也需要修改这些节点的/etc/lvm/lvm.conf，在过滤器中增加该类型的磁盘设备，例如：
filter = [ "a/sda/", "r/.*/"]

5）在存储节点安装配置cinder组件
yum install openstack-cinder targetcli python-keystone -y
yum install -y openstack-utils

6）在存储节点快速修改cinder配置
openstack-config --set  /etc/cinder/cinder.conf database connection  mysql+pymysql://cinder:cinder@controller/cinder
openstack-config --set  /etc/cinder/cinder.conf DEFAULT transport_url rabbit://openstack:000000@controller
openstack-config --set  /etc/cinder/cinder.conf DEFAULT auth_strategy  keystone 
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken www_authenticate_uri  http://controller:5000
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken auth_url  http://controller:5000
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken memcached_servers  controller:11211
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken auth_type  password
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken project_domain_name  default 
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken user_domain_name  default
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken project_name  service 
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken username  cinder
openstack-config --set  /etc/cinder/cinder.conf keystone_authtoken password  cinder
openstack-config --set  /etc/cinder/cinder.conf DEFAULT my_ip 192.168.56.123
openstack-config --set  /etc/cinder/cinder.conf lvm volume_driver cinder.volume.drivers.lvm.LVMVolumeDriver
openstack-config --set  /etc/cinder/cinder.conf lvm volume_group cinder-volumes
openstack-config --set  /etc/cinder/cinder.conf lvm iscsi_protocol  iscsi
openstack-config --set  /etc/cinder/cinder.conf lvm iscsi_helper  lioadm
openstack-config --set  /etc/cinder/cinder.conf DEFAULT enabled_backends  lvm
openstack-config --set  /etc/cinder/cinder.conf DEFAULT glance_api_servers  http://controller:9292
openstack-config --set  /etc/cinder/cinder.conf oslo_concurrency lock_path  /var/lib/cinder/tmp
# 如果存储节点是双网卡，选项my_ip需要配置存储节点的管理IP，否则配置本机IP

7）在存储节点启动cinder服务并配置开机自启动
# 需要启动2个服务
systemctl start openstack-cinder-volume.service target.service
systemctl status openstack-cinder-volume.service target.service
systemctl restart openstack-cinder-volume.service target.service

systemctl enable openstack-cinder-volume.service target.service
systemctl list-unit-files |grep openstack-cinder |grep enabled
systemctl list-unit-files |grep target.service |grep enabled
# 至此，在存储节点安装cinder服务就完成了

systemctl restart openstack-cinder-api  openstack-cinder-scheduler  openstack-cinder-volume


9.3.在控制节点进行验证
1）获取管理员变量
2）查看存储卷列表
openstack volume service list

例：
[root@openstack01 tools]# openstack volume service list
+------------------+-------------------------------+------+---------+-------+----------------------------+
| Binary           | Host                          | Zone | Status  | State | Updated At                 |
+------------------+-------------------------------+------+---------+-------+----------------------------+
| cinder-scheduler | openstack01.zuiyoujie.com     | nova | enabled | up    | 2018-10-31T10:55:19.000000 |
| cinder-volume    | openstack02.zuiyoujie.com@lvm | nova | enabled | up    | 2018-10-31T10:55:21.000000 |
+------------------+-------------------------------+------+---------+-------+----------------------------+
# 返回以上信息，表示cinder相关节点安装完成



swift --os-auth-url http://192.168.56.122:5000/v1 --auth-version 1 \
      --os-project-name service --os-project-domain-name default\
      --os-username admin --os-user-domain-name default\
      --os-password 000000 list


swift --os-auth-url http://192.168.56.122:5000/v3 --auth-version 3       --os-project-name admin --os-project-domain-name Default      --os-username admin --os-user-domain-name default      --os-password 000000 list


//--------------------------------------------------------------------
Install Heat

创建Heat用户
mysql -u root -p000000

CREATE DATABASE heat;

GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'localhost' \
  IDENTIFIED BY '000000';
GRANT ALL PRIVILEGES ON heat.* TO 'heat'@'%' \
  IDENTIFIED BY '000000';


. admin-openrc


openstack user create --domain default --password-prompt heat

openstack role add --project service --user heat admin


创建业务流程服务API终结点：

openstack service create --name heat \
  --description "Orchestration" orchestration

openstack service create --name heat-cfn \
  --description "Orchestration"  cloudformation

openstack endpoint create --region RegionOne \
  orchestration public http://controller:8004/v1/%\(tenant_id\)s

openstack endpoint create --region RegionOne \
  orchestration internal http://controller:8004/v1/%\(tenant_id\)s

openstack endpoint create --region RegionOne \
  orchestration admin http://controller:8004/v1/%\(tenant_id\)s

openstack endpoint create --region RegionOne \
  cloudformation public http://controller:8000/v1

openstack endpoint create --region RegionOne \
  cloudformation internal http://controller:8000/v1

openstack endpoint create --region RegionOne \
  cloudformation admin http://controller:8000/v1


创建包含项目和堆栈用户的热域：
openstack domain create --description "Stack projects and users" heat

创建heat域管理员用户以管理heat域中的项目和用户：
openstack user create --domain heat --password-prompt heat_domain_admin

将admin角色添加到heat域中的heat\u域管理员用户，以启用heat\u域管理员用户的管理堆栈管理权限：
openstack role add --domain heat --user-domain heat --user heat_domain_admin admin

openstack role create heat_stack_owner

将heat_stack_owner角色添加到演示项目和用户，以启用演示用户的堆栈管理：
openstack role add --project service --user admin heat_stack_owner

您必须将heat堆栈所有者角色添加到管理堆栈的每个用户。
openstack role create heat_stack_user

安装和配置组件
yum install -y openstack-heat-api openstack-heat-api-cfn \
  openstack-heat-engine

编辑/etc/heat/heat.conf文件并完成以下操作：
[database]
...
connection = mysql+pymysql://heat:000000@controller/heat

[DEFAULT]
...
transport_url = rabbit://openstack:000000@controller


[keystone_authtoken]
...
auth_uri = http://controller:5000
auth_url = http://controller:5000
memcached_servers = controller:11211
auth_type = password
project_domain_name = default
user_domain_name = default
project_name = service
username = heat
password = 000000

[trustee]
...
auth_type = password
auth_url = http://controller:35357
username = heat
password = 000000
user_domain_name = default

[clients_keystone]
...
auth_uri = http://controller:5000


[DEFAULT]
...
heat_metadata_server_url = http://controller:8000
heat_waitcondition_server_url = http://controller:8000/v1/waitcondition

[DEFAULT]
...
stack_domain_admin = heat_domain_admin
stack_domain_admin_password = 000000
stack_user_domain_name = heat


填充业务流程数据库：
su -s /bin/sh -c "heat-manage db_sync" heat


启动服务
systemctl enable openstack-heat-api.service \
  openstack-heat-api-cfn.service openstack-heat-engine.service
systemctl start openstack-heat-api.service \
  openstack-heat-api-cfn.service openstack-heat-engine.service


Heat Dashboard installation guide

pip install heat-dashboard

cp <heat-dashboard-dir>/enabled/_[1-9]*.py \
      /usr/share/openstack-dashboard/openstack_dashboard/local/enabled
/usr/lib/python2.7/site-packages/heat_dashboard/

POLICY_FILES['orchestration'] = '<heat-dashboard-dir>/conf/heat_policy.json'

$ cd <heat-dashboard-dir>
$ python ./manage.py compilemessages

$ cd <horizon-dir>
$ DJANGO_SETTINGS_MODULE=openstack_dashboard.settings python manage.py collectstatic --noinput
$ DJANGO_SETTINGS_MODULE=openstack_dashboard.settings python manage.py compress --force


$ sudo service apache2 restart
systemctl restart httpd


//---------------------------------------------------------------
OpenStack系统管理

使用仪表盘进行openstack管理
https://docs.openstack.org/zh_CN/user-guide/dashboard.html

使用命令行对openstack进行管理
https://docs.openstack.org/zh_CN/user-guide/cli.html

使用PySDK进行openstack管理
https://docs.openstack.org/zh_CN/user-guide/sdk.html

命令行速查
https://docs.openstack.org/zh_CN/user-guide/cli-cheat-sheet.html

Window 远程桌面 2008
114.116.43.75


Ubuntu install OpenStack huawei
http://114.116.42.121/dashboard/


mkdir /data
dd if=/dev/zero of=/data/swap bs=1024 count=6048000
mkswap /data/swap 
swapon /data/swap

/etc/fstab 
/data/swap     swap     swap    defaults  0 0


service apparmor stop
update-rc.d -f apparmor remove 

systemctl restart rabbitmq-server



cd /usr/lib/systemd/system/
 服务目录



git add --all


git commit -m "Initial commit"


git push -u origin master


centos查看网卡uuid

uuidgen eth1

- yum provides "*/nmcli"
- /etc/init.d/messagebus start
- /etc/init.d/NetworkManager start
- nmcli con



/etc/ssh/sshd_config

Ciphers aes128-cbc,aes192-cbc,aes256-cbc,aes128-ctr,aes192-ctr,aes256-ctr,3des-cbc,arcfour128,arcfour256,arcfour,blowfish-cbc,cast128-cbc

MACs hmac-md5,hmac-sha1,umac-64@openssh.com,hmac-ripemd160,hmac-sha1-96,hmac-md5-96

KexAlgorithms diffie-hellman-group1-sha1,diffie-hellman-group14-sha1,diffie-hellman-group-exchange-sha1,diffie-hellman-group-exchange-sha256,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group1-sha1,curve25519-sha256@libssh.org

ubuntu 网卡

vi /etc/network/interfaces

# The loopback network interface
auto lo
iface lo inet loopback

# The primary network interface
#auto enp0s3
#iface enp0s3 inet dhcp
auto enp0s3
iface enp0s3 inet static
address 192.168.56.128
netmask 255.255.255.0
gateway 192.168.56.1



VirtualBox中使用ubuntu-16.04.1安装devstack的Compute节点
1、Controller节点主机配置

　　4G内存，80G硬盘，2个网卡：bridged、internal network

　　bridged                ----连接API及管理网络，上网

　　internal network    ----连接租户网络，保证controller和compute的通信

2、OS安装

　　ubuntu-16.04.1-server-amd64，安装ssh和kvm（openssh-server和virtual machine host）组件

3、OS配置

允许root远程登录
　　同controller

网卡配置
1
2
3
4
5
6
7
8
9
10
11
12
vi /etc/network/interfaces
auto enp0s3
iface enp0s3 inet static
address 192.168.56.11
netmask 255.255.255.0
gateway 192.168.56.1
dns-nameservers 8.8.8.8
  
auto enp0s8
iface enp0s8 inet manual
  
systemctl restart networking.service    ---重启网络服务


更改apt源并进行update
　　同controller

4、部署openstack

 安装pip
　　apt install python-pip

下载openstack至/root/目录
　　git clone https://git.openstack.org/openstack-dev/devstack -b stable/newton

创建stack用户
　　/root/devstack/tools/create-stack-user.sh        ---创建用户

root用户下执行
mkdir ~/.pip&&vi ~/.pip/pip.conf

stack用户下执行
mkdir ~/.pip&&vi ~/.pip/pip.conf

[global]
index-url = https://pypi.douban.com/simple
download_cache = ~/.cache/pip
[install]
use-mirrors = true
mirrors = http://pypi.douban.com/



[[local|localrc]]
 
MULTI_HOST=true
# management & api network
HOST_IP=192.168.104.11
 
# Credentials
ADMIN_PASSWORD=admin
MYSQL_PASSWORD=secret
RABBIT_PASSWORD=secret
SERVICE_PASSWORD=secret
SERVICE_TOKEN=abcdefghijklmnopqrstuvwxyz
 
# Service information
SERVICE_HOST=192.168.104.10
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
Q_HOST=$SERVICE_HOST
KEYSTONE_AUTH_HOST=$SERVICE_HOST
KEYSTONE_SERVICE_HOST=$SERVICE_HOST
 
ENABLED_SERVICES=n-cpu,q-agt,neutron
Q_AGENT=linuxbridge
ENABLE_TENANT_VLANS=True
TENANT_VLAN_RANGE=3001:4000
PHYSICAL_NETWORK=default
 
# vnc config
NOVA_VNC_ENABLED=True
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html"
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
 
LOG_COLOR=True
LOGDIR=$DEST/logs
SCREEN_LOGDIR=$LOGDIR/screen
 
# use TryStack git mirror
GIT_BASE=http://git.trystack.cn
NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git
SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git
执行安装脚本
/opt/stack/devstack/stack.sh         

Usage
For Git Clone
 git clone http://git.trystack.cn/openstack/nova.git 
For DevStack
Add GIT_BASE, NOVNC_REPO and SPICE_REPO variables to local.conf file.

[[local|localrc]]

# use TryStack git mirror
GIT_BASE=http://git.trystack.cn
NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git
SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git


Ubuntu devstack install OpenStack
unstack.sh
stack.sh

预准备
ulimit -SHn 65536

/etc/security/limits.conf
* hard nofile 65536
* soft nofile 65536


vi /etc/systemd/system.conf

DefaultLimitNOFILE = 65536


root用户下执行
mkdir ~/.pip&&vi ~/.pip/pip.conf

stack用户下执行
mkdir ~/.pip&&vi ~/.pip/pip.conf

[global]
index-url = https://pypi.douban.com/simple
download_cache = ~/.cache/pip
[install]
use-mirrors = true
mirrors = http://pypi.douban.com/


设置主机名
hostnamectl set-hostname controller

添加主机映射
cat << EOF >> /etc/hosts
192.168.56.128 controller
EOF

禁用 Apparmor
service apparmor stop
update-rc.d -f apparmor remove 
apt-get remove apparmor apparmor-utils

service sendmail stop; update-rc.d -f sendmail remove


[global]
index-url = http://mirrors.aliyun.com/pypi/simple 
extra-index-url = https://pypi.tuna.tsinghua.edu.cn/simple
trusted-host = 
    mirrors.aliyun.com            
    pypi.tuna.tsinghua        
timeout = 120





fuel工具部署的openstack 环境的dashboard进行简体中文汉化

1）直接运行“apt-get install gettext”命令。

2）转到fuel已经配置好的中文配置文件夹下
cd /usr/share/openstack-dashboard/openstack_dashboard/locale/zh_CN/LC_MESSAGES

3）编译语言包，运行如下命令：msgfmt --statistics --verbose -o django.mo django.po会生成django.mo文件
 


4）链接语言包；先转到相应目录，然后连接语言包
a) cd /usr/lib/python2.7/dist-packages/horizon/locale/zh_CN/LC_MESSAGES
b) ln-s /usr/share/openstack-dashboard/openstack_dashboard/locale/zh_CN/LC_MESSAGES/django.mo django.mo 

5）重启服务
sudo service apache2 restart



https://cloud.tencent.com/act/southwest?from=10294




在devstack脚本中看到默认vg的创建过程
lib/lvm
function _create_lvm_volume_group

复制代码
truncate -s 15G vg_file
sudo losetup -f --show vg_file
/dev/loop0
sudo vgcreate stack-volumes-lvmdriver-1 /dev/loop0
  No physical volume label read from /dev/loop0
  Physical volume "/dev/loop0" successfully created
  Volume group "stack-volumes-lvmdriver-1" successfully created
复制代码

重启cinder-volume就可以。

created or not. If not execute the below commands:


[root@localhost ~]# 
losetup -f
/dev/loop0 
[root@localhost ~]# 
sudo losetup /dev/loop0 /opt/stack/data/stack-volumes-default-backing-file
[root@localhost ~]# 
losetup -f
/dev/loop1
[root@localhost ~]# 
sudo losetup /dev/loop1 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file
[root@localhost ~]# 
systemctl restart devstack@c-vol.service



[root@localhost ~]# losetup -f
/dev/loop0 
[root@localhost ~]# sudo losetup /dev/loop0 /opt/stack/data/stack-volumes-default-backing-file
[root@localhost ~]# losetup -f
/dev/loop1
[root@localhost ~]# sudo losetup /dev/loop1 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file
[root@localhost ~]# systemctl restart devstack@c-vol.service



增加交换空间
dd if=/dev/zero of=/data/swap bs=1024 count=4048000
mkswap /data/swap 
swapon /data/swap

/etc/fstab 
/data/swap     swap     swap    defaults  0 0


/dev/centos-swap swap    swap    defaults        0       0


mount -a

/etc/nova/nova.conf
vncserver_proxyclient_address=192.168.2.14






修改：
vi 15-horizon_vhost.conf
添加  ServerAlias 114.116.47.43


https://wsgzao.github.io/post/openstack/#





1、选择一台CentOS服务器，安装以下软件：

yum install yum-utils createrepo yum-plugin-priorities
 
yum install httpd
2、开启http服务

systemctl start httpd
3、获取repo文件并使用reposync同步源

yum install -y http://[rdo.fedorapeople.org/rdo-release.rpm](http://rdo.fedorapeople.org/rdo-release.rpm)
 
yum repolist #可以看到源的id列表
4、同步openstack-queen这个repo

cd /var/www/html/
reposync --repoid=openstack-queen
5、第一次同步时间较长，同步结束后

createrepo –update /var/www/html/openstack-queen
6、创建完成后，就可以使用web测试：http://[ip]/openstack-icehouse/

7、选择另外一台服务器作为客户机

cd /etc/yum.repos.d
mv CentOS-Base.repo CentOS-Base.repo_bak
cp CentOS-Media.repo CentOS-Media.repo_bak
vim CentOS-Media.repo
 
[openstack-queens]
name=OpenStack Queens Repository
baseurl=http://47.98.122.105/openstack-queens/
enabled=1
gpgcheck=0
至此，配置完成，清除缓存并查看软件包

yum clean all
yum list

远程windows主机
114.116.43.75


CentOS install openstack
114.116.47.43


Ubuntu install openstack
114.116.42.121

abc123ABC



$scp -r localfile.txt username@192.168.0.1:/home/username/
$scp -r username@192.168.0.1:/home/username/remotefile.txt



四川移动 +8613800280500 四川联通 +8613010811500
都+1


Ubuntu install OpenStack huawei
http://114.116.42.121/dashboard/



dd if=/dev/zero of=/data/swap bs=1024 count=4048000
mkswap /data/swap 
swapon /data/swap


/etc/fstab 
/data/swap     swap     swap    defaults  0 0


service apparmor stop
update-rc.d -f apparmor remove 

systemctl restart rabbitmq-server


cp /bin/systemctl /bin/mmsystemctl

cp /usr/bin/systemctl /usr/bin/mmsystemctl

cat << EOF >> /usr/bin/mmsystemctl.sh 
systemctl list-unit-files
EOF
chmod +x /usr/bin/mmsystemctl.sh

cat << EOF >> /usr/bin/opennetlist.sh 
openstack network list
EOF
chmod +x /usr/bin/opennetlist.sh




systemctl list-unit-files | grep enabled
mmsystemctl.sh 显示所有任务
mmsystemctl 原系统命令




ip netns list
ip netns exec qrouter-aca36d32-5819-4e7a-af1a-e9f3ebf88282 route -n
ip netns exec qrouter-aca36d32-5819-4e7a-af1a-e9f3ebf88282 route -n 
ip netns exec qrouter-aca36d32-5819-4e7a-af1a-e9f3ebf88282 ping 10.0.0.10

ip route add 172.24.4.0/24 dev br-ex

openstack security group rule create --proto icmp --dst-port 0 default
openstack security group rule create --proto tcp --dst-port 22 default
neutron net-create --provider:network_type flat --provider:physical_network extnet --router:external  --shared external_network

neutron subnet-create --name public_subnet --enable_dhcp=True --allocation-pool start=192.168.123.100,end=192.168.123.200 --gateway=192.168.123.2 external_network 192.168.123.0/24

neutron net-create private_network
neutron subnet-create --name private_subnet --allocation-pool start=10.10.1.100,end=10.10.1.200 --gateway=10.10.1.1 private_network 10.10.1.0/24

neutron router-create router1
neutron router-interface-add router1 private_subnet
neutron router-gateway-set router1 external_network

nova list
nova delete  public-instance

openstack router list

openstack router delete
openstack router delete router1


openstack router remove subnet


--gateway=172.24.5.1 external_network 172.24.5.0/24

neutron net-create --provider:network_type flat --provider:physical_network extnet --router:external  --shared external_network

neutron subnet-create --name public_subnet --enable_dhcp=True --allocation-pool start=172.24.4.10,end=172.24.4.100 external_network 172.24.4.0/24 


neutron router-create router1
neutron router-gateway-set router1 external_network


neutron net-list

nova boot --flavor m1.tiny --image cirros --nic net-id=205654b5-2c7c-44f3-a804-37d9f127d4b8 --security-group default public-instance


ifconfig eth0 172.16.25.125 netmask 255.255.255.224 broadcast 172.16.25.63


Make /etc/sysconfig/network-scripts/ifcfg-br-ex resemble:
DEVICE=br-ex
DEVICETYPE=ovs
TYPE=OVSBridge
BOOTPROTO=static
IPADDR=192.168.122.212 # Old eth0 IP since we want the network restart to not 
                       # kill the connection, otherwise pick something outside your dhcp range
NETMASK=255.255.255.0  # your netmask
GATEWAY=192.168.122.1  # your gateway
DNS1=192.168.122.1     # your nameserver
ONBOOT=yes


if on Centos7, the file could be /etc/sysconfig/network-scripts/ifcfg-enp2s0 and DEVICE should be enp2s0
DEVICE=eth0
TYPE=OVSPort
DEVICETYPE=ovs
OVS_BRIDGE=br-ex
ONBOOT=yes

创建 admin 项目
openstack project create --domain default --description "Admin Project" admin2

创建 admin 用户：
openstack user create --domain default --password-prompt admin2
123456

创建 admin 角色：
openstack role create admin2

添加``admin`` 角色到 admin 项目和用户上：
openstack role add --project admin2 --user admin admin2

Compute service(Nova)－15％
管理flavor

nova flavor-list
nova flavor-create m1.vcomputer 7 64 1 1

nova flavor-access-add 6 9467f30b8bba4770a06a687e4584636b

管理compute instance行为（如启动，关闭，终止）
glance image-list

neutron net-list

nova secgroup-list

nova list
nova show 3e4db956-2c72-4aa0-ac53-495d8550f684

关闭一个实例
nova stop 3e4db956-2c72-4aa0-ac53-495d8550f684 
nova start 3e4db956-2c72-4aa0-ac53-495d8550f684 

终止实例
nova delete 3e4db956-2c72-4aa0-ac53-495d8550f684 



启动一个新实例
nova boot --flavor m1.vcomputer --image cirros --nic net-id=f94ccd9862c24bcbb5f01ec5d61fc8c2 --security-group default public-instance

openstack network list
nova boot --flavor m1.vcomputer --image cirros --nic net-id=a046ae67-679f-4575-afb8-aa122b9a4db2 --security-group default public-instance


管理Nova用户密钥对(keypair)
nova keypair-add --pub-key /root/.ssh/id_rsa.pub terrykey
ssh-keygen 命令
nova keypair-list

当 instance 分配了浮动 IP 后, 允许 server 能够直接访问 instance 时候, 才可以利用密钥配对进行访问
假如通过 novnc 进行访问时候, 仍然需要通过 root/password 方法进行访问


配置一个拥有floating IP的实例
# 查看可用实例类型
openstack flavor list

# 查看可用启动镜像
openstack image list

# 查看可用网络
openstack network list

# 查看可用安全组
openstack security group list

# 启动一个实例
openstack server create --flavor m1.medium --image win7_x64 --nic net-id=6e85dfe1-f976-4407-87ae-a217a46c9dff --security-group default test-instance

# 查看创建的实例
openstack server list

# 查看浏览器vnc窗口访问连接
openstack console url show test-instance

# 查看浮动IP池
openstack floating ip pool list

# 从浮动IP池中获取一个浮动IP
openstack floating ip create external_network

# 查看已经获取的浮动IP
openstack floating ip list

# 绑定浮动IP给实例
openstack ip floating add 192.168.10.129 test-instance

openstack floating ip delete 192.168.2.12


# 使用nova命令启动实例
nova boot --flavor m1.medium --image win7_x64 --nic net-id=6e85dfe1-f976-4407-87ae-a217a46c9dff --security-group default test-instance



管理项目的安全组规则
openstack security group list
openstack security group create SECURITY_GROUP_NAME --description GROUP_DESCRIPTION
openstack security group delete SECURITY_GROUP_NAME
openstack security group delete 196b5736-6e69-4622-aea4-408b8aa09b10


openstack security group rule list SECURITY_GROUP_NAME
openstack security group rule create SECURITY_GROUP_NAME \
      --protocol tcp --dst-port 22:22 --remote-ip 0.0.0.0/0

openstack security group rule create SECURITY_GROUP_NAME \
      --protocol tcp --dst-port 22:22 --remote-group SOURCE_GROUP_NAME

openstack security group rule create --protocol icmp \
  SECURITY_GROUP_NAME

openstack security group rule create --protocol icmp \
  --remote-group SOURCE_GROUP_NAME SECURITY_GROUP

openstack security group rule create --protocol udp \
  --dst-port 53:53 SECURITY_GROUP

openstack security group rule create --protocol udp \
  --dst-port 53:53 --remote-group SOURCE_GROUP_NAME SECURITY_GROUP

openstack security group rule delete RULE_ID

管理实例快照
openstack server list
openstack server stop myInstance
openstack server list
openstack server image create myInstance --name myInstanceSnapshot
openstack image list

openstack image list
openstack image save --file snapshot.raw f30b204e-1ce6-40e7-b8d9-b353d4d84e7d

openstack image create NEW_IMAGE_NAME --container-format bare --disk-format qcow2 --file IMAGE_URL

openstack server create --flavor m1.tiny --image myInstanceSnapshot myNewInstance

管理Nova计算节点
管理配额
获取Nova数据（主机数，服务数，租户数)
nova quota-defaults
nova quota-class-update default key value
nova quota-class-update default --instances 15

tenant=$(openstack project list | awk '/租户名称/ {print $2}')
nova quota-show --tenant $tenant
nova quota-update --floating-ips 20 $tenant
nova quota-show --tenant $tenant
cinder quota-defaults $tenant

Object Storage service(Swift)－10%
管理对象存储的入口
OpenStack对象存储（SWIFT）用于冗余，可扩展的使用标准化的服务器集群存储访问数据字节的数据存储仓库。OpenStack对象存储是一个用于大量的静态数据的长期的存储系统，它可以检索和更新


管理到期的对象
管理存储规则
监控对象存储的可用空间
检验对象存储的运行
管理在对象存储中的容器的权限



Block Storage service(Cinder)－10%
管理卷
创建块存储的卷组
创建一个新的卷并将其安装到Nova实例上
管理配额
管理卷的配额
管理卷的备份
备份和恢复卷
管理卷快照（如take,list,recover)
检验块存储具备快照功能
做卷的快照
管理卷的加密
建立存储池
监控块存储装置的预留空间
分析卷大小的差异



Networking service(Neutron)－16%
管理网络资源（如路由,子网）
创建外部网络
创建网络和子网
创建路由
在虚拟环境中管理网络服务
管理安全组规则
管理配额
检验网络服务的运行
管理compute instance上的网络交互界面
做卷的租户网络故障诊断和排除（enter namespace, run tcpdump等）



Orchestration Service(Heat)－8%
使用Heat/Orchestration模版启动一个Resource stack（如存储、网络、计算）
使用Heat/Orchestration CLI和Dashboard
检验Heat/Orchestration stack在工作中
检验Heat/Orchestration stack的运行
针对一个特定的场景，创建相匹配的Heat/Orchestration模版
更新一个stack
获取一个stack的详细信息


Troubleshooting－13%
分析日志文件
使备份OpenStack实例使用的数据库
集中并分析日志（如/var/log/COMPONENT_NAME, Database Server, Messaging Server, Web Server, syslog）
分析数据库服务器
分析Host/Guest操作系统和实例的状态
分析消息服务器
分析元数据服务器
分析网络状态（物理和虚拟）
分析存储状态（本地存储，块存储&对象存储）
管理OpenStack服务
诊断服务突发故障
理解OpenStack环境（控制器，计算，存储和网络节点）
通过集中式的日志系统直达日志文件
备份和修复OpenStack实例
故障排除网络性能


Image management(Glance)－10%
为OpenStack实例部署一个新镜像
管理镜像类型和后端
管理镜像（如添加，更新，移除）
检验镜像服务的运行

各模块的比重：3% Know OpenStack＋12%Keystone+3%Horizon +15%Nova+10%Swift+10%Cinder＋15%Neutron＋3%Ceilometer＋6%Heat＋13%Troubleshooting＋10%Glance＝ 100%OpenStacker！



控制节点：
service openstack-nova-api restart
service openstack-nova-scheduler restart
service openstack-nova-conductor restart
service neutron-server restart

网络节点：
service openvswitch restart
service neutron-openvswitch-agent restart（fuel控制节点默认stop）
service neutron-l3-agent restart（fuel控制节点默认stop）
service neutron-dhcp-agent restart（fuel控制节点默认stop）
service neutron-metadata-agent restart（fuel控制节点默认stop）

计算节点：
service neutron-openvswitch-agent restart
service openvswitch restart


-------------------------------------------------------------------
【重启cinder服务】

控制节点：
service openstack-cinder-api restart
service openstack-cinder-scheduler restart

存储节点：
service openstack-cinder-volume restart
-------------------------------------------------------------------
【重启glance服务】

控制节点：
service openstack-glance-api restart
service openstack-glance-registry restart
-------------------------------------------------------------------
【重启swift服务】

控制节点：
service openstack-swift-proxy restart
service memcached restart

存储节点：
service openstack-swift-account restart
service openstack-swift-account-auditor restart
service openstack-swift-account-reaper restart
service openstack-swift-account-replicator restart
service openstack-swift-container restart
service openstack-swift-container-auditor restart
service openstack-swift-container-replicator restart
service openstack-swift-container-updater restart
service openstack-swift-object restart
service openstack-swift-object-auditor restart
service openstack-swift-object-replicator restart
service openstack-swift-object-updater restart
-------------------------------------------------------------------
【重启nova服务】

控制节点：
service openstack-nova-api restart
service openstack-nova-cert restart
service openstack-nova-consoleauth restart
service openstack-nova-scheduler restart
service openstack-nova-conductor restart
service openstack-nova-novncproxy restart

计算节点：
service libvirtd restart
service openstack-nova-compute restart

abc123ABC





$scp -r localfile.txt username@192.168.0.1:/home/username/
$scp -r username@192.168.0.1:/home/username/remotefile.txt

	





这是可供参考的常用命令列表

认证 (keystone)¶
列出所有的用户

$ openstack user list
列出认证服务目录

$ openstack catalog list
镜像（glance）¶
列出您可以访问的镜像

$ openstack image list
删除指定的镜像

$ openstack image delete IMAGE
描述一个指定的镜像

$ openstack image show IMAGE
更新镜像

$ openstack image set IMAGE
上传内核镜像

$ openstack image create "cirros-threepart-kernel" \
  --disk-format aki --container-format aki --public \
  --file ~/images/cirros-0.3.5-x86_64-kernel
上传RAM镜像

$ openstack image create "cirros-threepart-ramdisk" \
  --disk-format ari --container-format ari --public \
  --file ~/images/cirros-0.3.5-x86_64-initramfs
上传第三方镜像

$ openstack image create "cirros-threepart" --disk-format ami \
  --container-format ami --public \
  --property kernel_id=$KID-property ramdisk_id=$RID \
  --file ~/images/cirros-0.3.5-x86_64-rootfs.img
注册raw镜像

$ openstack image create "cirros-raw" --disk-format raw \
  --container-format bare --public \
  --file ~/images/cirros-0.3.5-x86_64-disk.img
计算 (nova)¶
列出实例，核实实例状态

$ openstack server list
列出镜像

$ openstack image list
Create a flavor named m1.tiny

$ openstack flavor create --ram 512 --disk 1 --vcpus 1 m1.tiny
列出规格类型

$ openstack flavor list
用类型和镜像名称(如果名称唯一)来启动云主机

$ openstack server create --image IMAGE --flavor FLAVOR INSTANCE_NAME
$ openstack server create --image cirros-0.3.5-x86_64-uec --flavor m1.tiny \
  MyFirstInstance
Log in to the instance (from Linux)

 注解

The ip command is available only on Linux. Using ip netns provides your environment a copy of the network stack with its own routes, firewall rules, and network devices for better troubleshooting.

# ip netns
# ip netns exec NETNS_NAME ssh USER@SERVER
# ip netns exec qdhcp-6021a3b4-8587-4f9c-8064-0103885dfba2 \
  ssh cirros@10.0.0.2
 注解

In CirrOS, the password for user cirros is cubswin:). For any other operating system, use SSH keys.

Log in to the instance with a public IP address (from Mac)

$ ssh cloud-user@128.107.37.150
显示实例详细信息

openstack server show NAME
$ openstack server show MyFirstInstance
查看云主机的控制台日志

openstack console log show MyFirstInstance
设置云主机的元数据

$ nova meta volumeTwoImage set newmeta='my meta data'
创建一个实例快照

$ openstack image create volumeTwoImage snapshotOfVolumeImage
$ openstack image show snapshotOfVolumeImage
实例的暂停、挂起、停止、救援、调整规格、重建、重启¶
暂停

$ openstack server pause NAME
$ openstack server pause volumeTwoImage
取消挂起

$ openstack server unpause NAME
挂起

$ openstack server suspend NAME
Unsuspend

$ openstack server resume NAME
关机

$ openstack server stop NAME
开始

$ openstack server start NAME
恢复

$ openstack server rescue NAME
$ openstack server rescue NAME --rescue_image_ref RESCUE_IMAGE
调整大小

$ openstack server resize NAME FLAVOR
$ openstack server resize my-pem-server m1.small
$ openstack server resize --confirm my-pem-server1
重建

$ openstack server rebuild NAME IMAGE
$ openstack server rebuild newtinny cirros-qcow2
重启

$ openstack server reboot NAME
$ openstack server reboot newtinny
将用户数据和文件注入到实例

$ openstack server create --user-data FILE INSTANCE
$ openstack server create --user-data userdata.txt --image cirros-qcow2 \
  --flavor m1.tiny MyUserdataInstance2
使用ssh连接到实例，查看``/var/lib/cloud``验证文件是否成功注入

给实例注入一个密钥对并通过密钥对来访问实例

创建秘钥对

$ openstack keypair create test > test.pem
$ chmod 600 test.pem
启动实例

$ openstack server create --image cirros-0.3.5-x86_64 --flavor m1.small \
  --key-name test MyFirstServer
使用ssh连接到实例

# ip netns exec qdhcp-98f09f1e-64c4-4301-a897-5067ee6d544f \
  ssh -i test.pem cirros@10.0.0.4
管理安全组

在默认的安全组中，添加ping和SSH规则

$ openstack security group rule create default \
    --remote-group default --protocol icmp
$ openstack security group rule create default \
    --remote-group default --dst-port 22
网络 (neutron)¶
创建网络

$ openstack network create NETWORK_NAME
创建子网

$ openstack subnet create --subnet-pool SUBNET --network NETWORK SUBNET_NAME
$ openstack subnet create --subnet-pool 10.0.0.0/29 --network net1 subnet1
块存储(cinder)¶
用于管理连接到实例的卷和卷快照。

创建一个新卷

$ openstack volume create --size SIZE_IN_GB NAME
$ openstack volume create --size 1 MyFirstVolume
启动实例并将它链接到卷上

$ openstack server create --image cirros-qcow2 --flavor m1.tiny MyVolumeInstance
列出所有卷，注意卷状态

$ openstack volume list
当实例为正常状态且卷为可用状态时，将卷连接到实例。

$ openstack server add volume INSTANCE_ID VOLUME_ID
$ openstack server add volume MyVolumeInstance 573e024d-5235-49ce-8332-be1576d323f8
 注解

在Xen Hypervisor可以指定具体的设备名，而不使用自动分配的名称，例如：

$ openstack server add volume --device /dev/vdb MyVolumeInstance 573e024d..1576d323f8

This is not currently possible when using non-Xen hypervisors with OpenStack.
登陆进实例之后管理卷组

列出存储器

# fdisk -l
在卷上建立文件系统

# mkfs.ext3 /dev/vdb
创建一个挂载点

# mkdir /myspace
在挂载点挂载卷

# mount /dev/vdb /myspace
在卷上创建一个文件

# touch /myspace/helloworld.txt
# ls /myspace
卸载卷

# umount /myspace
对象存储(Swift)¶
展示账户，容器以及对象的信息

$ swift stat
$ swift stat ACCOUNT
$ swift stat CONTAINER
$ swift stat OBJECT
列出容器

$ swift list


openstack compute service list

查看网络服务的命令
openstack network agent list


展示所有租户
openstack project list

创建租户
openstack project create --description 'Project description' project-name


更新租户名称
openstack project set ID/name --name project-new

删除租户
openstack project delete project-name

展示所有用户
openstack user list

添加用户
openstack user create --password name 该方式指定密码字符串
openstack user create --password-prompt name 该方式交互式填入密码

禁止用户
openstack user set user_name --disable

启用用户
openstack user set user_name --enable

更新用户名
openstack user set user_name --name username_new

删除用户
openstack user delete user_name

展示所有角色
openstack role list


创建新的角色
openstack role create new_role

综合：将某某用户添加到某某角色下
openstack role add --user user_name --project project_name role_name

显示结果
openstack role list --user user_name --peoject project_name


创建一个镜像
openstack image create --disk-format 参数 --public --file 参数 image_name

查看安全组
openstack group list

增加安全组
openstack group create group_name --description ""


查看openstack环境主机数量
openstack host list

查看某一台主机资源情况
openstack host show host_name

查看主机网络服务
openstack network agent list

查看端口信息
openstack port list

查看网络信息
openstack network list

创建外网
openstack network create --external outsidenet

创建外网子网
openstack subnet create --allocation-pool start=88.88.88.2,end=88.88.88.80 --subnet-range 88.88.88.0/24 --network outsidenet subnet_name

创建内网
openstack network create --internal --provider-network-type gre insidenet

创建内网子网
openstack subnet create --subnet-range 20.0.0.0/24 --network insidenet --dns-nameserver 114.114.114.114 provider_subent

创建路由器
openstack router create router_name

路由器连接子网
opens= tack router add subnet router_demo insidesubnet

路由器设置网关
openstack router set --external-gateway outsidenet router_name

3.5 虚机管理
查看虚拟机数量
openstack server list

创建虚拟机
启动一个虚拟机必要的信息是镜像，规格，网络。从前面学习到的image操作中找到镜像，从flavor操作中找到合适的规格，从网络操作中找到网络。
openstack server create --image cirros --flavor small --nic net-id=insidenet VM_name

虚拟机暂停
openstack server pause VM_name

虚拟机启动
openstack server unpause VM_name

虚拟机重启
openstack server reboot VM_name

OPENSTACK 命令行客户端
https://docs.openstack.org/zh_CN/user-guide/cli.html








1、选择一台CentOS服务器，安装以下软件：
yum install yum-utils createrepo yum-plugin-priorities
yum install httpd
2、开启http服务

systemctl start httpd

3、获取repo文件并使用reposync同步源
yum install -y http://[rdo.fedorapeople.org/rdo-release.rpm](http://rdo.fedorapeople.org/rdo-release.rpm)
yum repolist #可以看到源的id列表
4、同步openstack-queen这个repo
cd /var/www/html/
reposync --repoid=openstack-queen
5、第一次同步时间较长，同步结束后

createrepo –update /var/www/html/openstack-queen

6、创建完成后，就可以使用web测试：http://[ip]/openstack-icehouse/
http://114.115.235.66/openstack-rocky/

http://192.168.0.61/openstack-rocky/


7、选择另外一台服务器作为客户机
cd /etc/yum.repos.d
mv CentOS-Base.repo CentOS-Base.repo_bak
cp CentOS-Media.repo CentOS-Media.repo_bak
vim CentOS-Media.repo
[openstack-queens]
name=OpenStack Queens Repository
baseurl=http://47.98.122.105/openstack-queens/
enabled=1
gpgcheck=0
至此，配置完成，清除缓存并查看软件包
yum clean all
yum list




HOT模板举例-部署一个简单实例
heat_template_version: 2015-04-30  # heat模板版本必须有

description: #描述项是可选的，建议添加详细描述

resources:  # resources选项是必须的

  my_instance:

    type: OS::Nova::Server

    properties:

      key_name: my_key

      image: ubuntu-trusty-x86_64

      flavor: m1.small
 

5，模板举例-使用parameters参数
heat_template_version: 2015-04-30

description: Simple template to deploy a single compute instance

parameters:

  image_id:

    type: string

    label: Image ID

    description: Image to be used for compute instance

  flavor:

    type: string

    label: Instance Type

    description: Type of instance (flavor) to be used

    default: m1.small  # 如果用户没有为参数定义值，则在stack部署期间使用默认值。 如果模板没有为参数定义默认值，那么用户必须定义该值，否则stack创建失败。

    constraints:

      - allowed_values: [ m1.medium, m1.large, m1.xlarge ] # 约束列表限制输入参数的值

        description: Value must be one of m1.medium, m1.large or m1.xlarge.

resources:

  my_instance:

    type: OS::Nova::Server

    properties:

      image: { get_param: image_id }

      flavor: { get_param: flavor }

 

二，Heat功能应用
 

1，stack核心命令
heat stack-create / openstack stack create

heat stack-update / openstack stack update

heat stack-list / openstack stack list

heat stack-show / openstack stack show

heat stack-delete / openstack stack delete

heat output-list / openstack stack output list

heat output-show / openstack stack output show

 

2，stack其它命令
 

heat stack-abandon / openstack stack abandon

heat stack-adopt / openstack stack adopt

heat stack-cancel-update / openstack stack update cancel

heat stack-preview / openstack stack update --dry-run

heat action-check / openstack stack check

heat action-resume / openstack stack resume

heat action-suspend / openstack stack suspend

heat hook-clear / openstack stack hook clear

heat hook-poll / openstack stack hook poll

 

3，resource相关命令
heat resource-list / openstack stack resource list

heat resource-metadata / openstack stack resource metadata show

heat resource-show / openstack stack resource show

heat resource-signal / openstack stack resource signal

heat resource-type-list / openstack orchestration resource type list

heat resource-type-show / openstack orchestration resource type show

 

4，template相关命令
heat template-show  / openstack stack template show

heat template-validate  / openstack stack create --dry-run

heat template-version-list / openstack orchestration template version list

heat resource-type-template / openstack orchestration resource type show --format (hot|cfn)

 

5，event相关命令
heat event-list / openstack stack event list

heat event-show / openstack stack event show

 

6，软件配置命令
heat config-create / openstack software config create

heat config-delete / openstack software config delete

heat config-show / openstack software config show

heat config-list / openstack software config list

heat deployment-create / openstack software deployment create

heat deployment-delete / openstack software deployment delete

heat deployment-list / openstack software deployment list

heat deployment-metadata-show / openstack software deployment metadata show

heat deployment-output-show / openstack software deployment output show

heat deployment-show / openstack software deployment show

 

7，stack快照命令
heat stack-restore / openstack stack snapshot restore

heat stack-snapshot / openstack stack snapshot create

heat snapshot-delete / openstack stack snapshot delete

heat snapshot-list / openstack stack snapshot list

heat snapshot-show / openstack stack snapshot show

 

8，其它命令
heat build-info / openstack orchestration build-info

heat service-list / openstack service list (need to integrate with existing command)

 

常见操作链接：

https://docs.openstack.org/python-heatclient/latest/cli/index.html
https://specs.openstack.org/openstack/heat-specs/specs/mitaka/python-openstackclient.html

source devstatck/openrc admin admin


screen -x stack
sudo journalctl -f -u devstack@n-cond.service
sudo journalctl -f -u devstack@h*
tail -f /var/log/syslog


每个虚拟机对应一个qemu-kvm进程,同其他进程一样被调度。
ps -ef|grep qemu
virsh list

每个虚拟进会产生多个线程（cpu）在物理cpu上运行。
超配：虚拟机cpu数 > 物理cpu数。

KVM负责实现VA（虚拟内存）-》PA（物理内存）-》 MA（机器内存）的地址映射。
内存也可以超配，虚拟机内存数 大于 实际内存数。

sudo ls -l /var/lib/libvirt/images/

sudo vgdisplay

linux bridge 是linux上tcpip的二层协议交换设备，类似二层交换机或hub。

eth0 接收后 发送 br0， br0在转发给vnet0. VM1虚拟机1就能接收到外网数据。

LAN （local area network）本地局域网，使用hub、switch连接lan中的计算机。

VLAN是 Virtual Lan

Linux bridge + VLAN=虚拟交换机

OpenStack 是分布式系统，服务可以分布部署，组件也可以分布部署。


image存储位置
filesystem_store_datadir=/var/lib/glance/images/
/etc/glance/glance-api.conf


nova-conductor 负责管理数据库


nova.conf
compute_driver=libvirt.LibvirtDriver

虚拟机资源信息
novish nodeinfo

/etc/nova/nova.conf
instance_spath = /opt/stack/data/nova/instances
/var/lib/nova/instances/0185fefe-bdca-4a0a-885b-db1ba259a862
base_dir_name = _base

/var/lib/nova 是nova虚拟机存放目录
/opt/stack/data/nova 是ubuntu虚拟机存放目录

cinder 存在于那些节点
cinder service-list


more /etc/cinder/cinder.conf |grep volume_
volume_driver = cinder.volume.drivers.lvm.LVMVolumeDriver


os endpoint list
os endpoint show f71f85e9532246668fa15e7e7c8a7726


逻辑卷管理LVM是一个多才多艺的硬盘系统工具
传统分区使用固定大小分区，重新调整大小十分麻烦。但是，LVM可以创建和管理“逻辑”卷，而不是直接使用物理硬盘。

sudo journalctl -f -u devstack@n-cond.service

cinder 驱动

centos
/usr/lib/python2.7/site-packages/cinder/volume/drivers


ubuntu 
root@xihua3-0004:/opt/stack/devstack# ls -l /opt/stack/cinder/cinder/volume/drivers/
total 1072
drwxr-xr-x 2 stack stack   4096 Feb 10 22:01 dothill
-rw-r--r-- 1 stack stack  42105 Feb 10 22:01 drbdmanagedrv.py
....
创建卷源码
cd /opt/stack/cinder/cinder/volume/flows/api/
create_volume.py



yuantong
804317716173670838










https://blog.csdn.net/wllabs/article/details/79064094


FENET是一种安全的消息传递格式，已被国外的一些公司用于API token中。解决了OpenStack面临的许多相同问题。它们的非持久的、轻量级的特性可以降低运行云所需的开销。

fernet_token格式如下：

gAAAAABaVhKmk-inDnXQjDU-w6bw4BHhVhTAIOIspvGV6vZXHh8P9Kd6FMYdXeR6R8z_JfccBZxyTfooHvomBGhsrodIhWpAkL3lf-bqsAxr9byzPPQyyoHfp_IlOh2OgSngyt9NFVu39Sj4CM_vcInpBOCdMzMeGT7D0BRpyDC-6ziRpTmBlak
 

 

非持久化：不像UUID的token，fernet tokens 不需要被持久化到数据库。

是的!这意味着这个token将是永远是可以用的：

> SELECT * FROM `token`;
Empty set (0.00 sec)
 

 

轻量级：与 PKI token 相比，他们比较短，偏向于最小的身份信息和动态授权环境。

它们通常在180到240字节的范围内。从客户端的角度来看，您可以将它们完全视为 UUID token。

 

对称加密：Fernet 使用 AES-CBC 加密，并使用 SHA256 HMAC 进行签名。

简而言之，OpenStack的身份验证和授权元数据都被打包成一个包，然后将其加密并签名为 Fernet token。 OpenStack 支持 三阶段 密钥循环模型，能有效支持密钥的轮询切换和替代。

 

Fernet tokens 包含些什么？
Fernet tokens 包含最少的身份和授权信息：刚好可以让 keystone 实现完整的 token 验证。

在一般情况下（project-scoped token），一个token 会包含一个user id 和 一个 project id，以及一些元数据，如令牌创建时间戳，token时间周期，审计ID和认证方法。 domain-scoped token 会包含 domain ID而不是 project ID。 基于信任的token 将包含信任标识。 基于联合的 token 将包含 有关联盟本身的信息。

除了 token 创建时间戳以外，所有这些都是加密的，因为它是 fernet token 加密消息 的一部分。 fernet 做为包含这些信息的载体，fernet的更多细节可以参考这个https://github.com/fernet/spec/blob/master/Spec.md。

如果你想查看一个 fernet 内容，在打包和解包中你将会体会到fernet token 有多复杂和精密。

 

Fernet令牌有多大？
下面可以看一个 project-scoped Fernet token：

gAAAAABaVhKmk-inDnXQjDU-w6bw4BHhVhTAIOIspvGV6vZXHh8P9Kd6FMYdXeR6R8z_JfccBZxyTfooHvomBGhsrodIhWpAkL3lf-bqsAxr9byzPPQyyoHfp_IlOh2OgSngyt9NFVu39Sj4CM_vcInpBOCdMzMeGT7D0BRpyDC-6ziRpTmBlak
上面就只有186个字节，通常都会控制在255字节内。虽然255字节没有硬性限制，并且很长的token虽然存储到数据库里不是什么问题，但是长的 token 用户体验不好。 也有不好的事情，比如一些非UUID的用户可能会有额外的安全风险。

更另人吃惊的是，如果一个用户属于多个组，有可能是属于无穷多的组的时候，这个fernet token就也会相应的很庞大，不过现实私有云环境中，也不太可能一个用户属于3个组以上。

不过如果你将user 分配到二个不同的组，你很快就会看到 Fernet token 的长度迅速增加。

 

Fernet令牌可以离线验证吗？
可以进行一些离线验证，例如验证 token 是否是正确的 Fernet token，并验证创建日期。 但是，如果没有签名密钥或加密密钥，就不能离线验证了。

 

性能怎么样？
Fernet token 的性能比 UUID token 快 85％，比 PKI token 快89％。 

 

怎么配置并使用 Fernet？
只需要以下几步。

1，通过更改 keystone.conf 中[token]下的 provider：

[token]
provider = fernet
 

2，初始化密钥库：

mkdir /etc/keystone/fernet-keys/
keystone-manage fernet_setup

初始化密钥库后，会在 key_repository 里生成一对密钥：一个staged key 用于解密tokens，一个 primary key 用于加密token。

 

3，在部署多节点 keystone 集群时，这时候需要将  key_repository /etc/keystone/fernet-keys/  里的目录和文件 复制到每个其他的keystone节点相对应的目录上。这样确实可以用 fernet token 可以在不同的节点上使用相当的密钥进行解密。



[root@controller ~]# python token_test.py gAAAAABcZ4HeriL4pF1UsZWPZEzEyrEtGYPQVupehNnsiD6_GY1dvu5mnZAaHH8T8X4VQeoWUAUB3pUzKofh89AmCkPKcWenyqJFOxtuIjVsQMd68F7mfzKrhl4aerx0PBuvhnk3Omh9_6R5BxrZIqTxDD06tp7wFUEnu1XhwCP0dNLgdK0lSkY
raw token info:
[2, [True, '\xffFA\x84\x80\xffN\xc1\xb9\xce\x90A7q\xaf\xde'], 2, [True, '\x05u0\xcdA\xcdJ\xee\xa9la\x19\xd1a?\xa9'], 1550290926.0, ['i\x81\x07\xb9\xd3\xa5KA\xb2r\xa0\x87Y\x1dR.']]
=================================================
unpack token:
user_id = ff46418480ff4ec1b9ce90413771afde
project_id = 057530cd41cd4aeea96c6119d1613fa9
expired_at = 2019-02-16 12:22:06
['aYEHudOlS0GycqCHWR1SLg']
[root@controller ~]# 

[root@controller ~]# openstack user list
+----------------------------------+-----------+
| ID                               | Name      |
+----------------------------------+-----------+
| 432f99f94de7444a9d9c71ab4231b59d | neutron   |
| 5c7bcc9560ee44d298191ba768c235a5 | glance    |
| 8d4732ed2fee49c7ab4c1e51c41e6bfd | nova      |
| ada067ef765e42298b090cf946cc108b | myuser    |
| af09bef7ba6849e7849dea2817f7a1bf | placement |
| b25d1ac335694d9fafc209fe2bf1f59c | cinder    |
| ff46418480ff4ec1b9ce90413771afde | admin     |
+----------------------------------+-----------+

[root@controller ~]# openstack project list
+----------------------------------+-----------+
| ID                               | Name      |
+----------------------------------+-----------+
| 057530cd41cd4aeea96c6119d1613fa9 | admin     |
| 0dc85e9b92244c82bf70d4e218578aed | service   |
| 471f29df1f73428c9b5fdd8421e42feb | myproject |
+----------------------------------+-----------+



腾讯将加入 OpenStack 基金会 ，成为黄金会员。
腾讯的 TStack 私有云已经运行着 14 个 OpenStack 集群，总计有 6000 个节点。
这样的配置支持约 1 亿用户，腾讯宣布这一平台的可用性达到 99.99%。
腾讯是全球最大的 OpenStack 用户之一。




Create initial networks¶
The configuration supports one flat or multiple VLAN provider networks. For simplicity, the following procedure creates one flat provider network.

Source the administrative project credentials.

Create a flat network.

openstack network create --share --provider-physical-network provider \
  --provider-network-type flat provider1

$ openstack network create --share --provider-physical-network provider \
  --provider-network-type flat provider1
+---------------------------+-----------+-
| Field                     | Value     |
+---------------------------+-----------+
| admin_state_up            | UP        |
| mtu                       | 1500      |
| name                      | provider1 |
| port_security_enabled     | True      |
| provider:network_type     | flat      |
| provider:physical_network | provider  |
| provider:segmentation_id  | None      |
| router:external           | Internal  |
| shared                    | True      |
| status                    | ACTIVE    |
+---------------------------+-----------+
 Note

The share option allows any project to use this network. To limit access to provider networks, see Role-Based Access Control (RBAC).

 Note

To create a VLAN network instead of a flat network, change --provider:network_type flat to --provider-network-type vlan and add --provider-segment with a value referencing the VLAN ID.

Create a IPv4 subnet on the provider network.

$ openstack subnet create --subnet-range 203.0.113.0/24 --gateway 203.0.113.1 \
  --network provider1 --allocation-pool start=203.0.113.11,end=203.0.113.250 \
  --dns-nameserver 8.8.4.4 provider1-v4
+-------------------+----------------------------+
| Field             | Value                      |
+-------------------+----------------------------+
| allocation_pools  | 203.0.113.11-203.0.113.250 |
| cidr              | 203.0.113.0/24             |
| dns_nameservers   | 8.8.4.4                    |
| enable_dhcp       | True                       |
| gateway_ip        | 203.0.113.1                |
| ip_version        | 4                          |
| name              | provider1-v4               |
+-------------------+----------------------------+
 Note

Enabling DHCP causes the Networking service to provide DHCP which can interfere with existing DHCP services on the physical network infrastructure.

Create a IPv6 subnet on the provider network.

$ openstack subnet create --subnet-range fd00:203:0:113::/64 --gateway fd00:203:0:113::1 \
  --ip-version 6 --ipv6-address-mode slaac --network provider1 \
  --dns-nameserver 2001:4860:4860::8844 provider1-v6
+-------------------+------------------------------------------------------+
| Field             | Value                                                |
+-------------------+------------------------------------------------------+
| allocation_pools  | fd00:203:0:113::2-fd00:203:0:113:ffff:ffff:ffff:ffff |
| cidr              | fd00:203:0:113::/64                                  |
| dns_nameservers   | 2001:4860:4860::8844                                 |
| enable_dhcp       | True                                                 |
| gateway_ip        | fd00:203:0:113::1                                    |
| ip_version        | 6                                                    |
| ipv6_address_mode | slaac                                                |
| ipv6_ra_mode      | None                                                 |
| name              | provider1-v6                                         |
+-------------------+------------------------------------------------------+
 Note

The Networking service uses the layer-3 agent to provide router advertisement. Provider networks rely on physical network infrastructure for layer-3 services rather than the layer-3 agent. Thus, the physical network infrastructure must provide router advertisement on provider networks for proper operation of IPv6.

Verify network operation¶
On each compute node, verify creation of the qdhcp namespace.

# ip netns
qdhcp-8b868082-e312-4110-8627-298109d4401c
Source a regular (non-administrative) project credentials.

Create the appropriate security group rules to allow ping and SSH access instances using the network.

$ openstack security group rule create --proto icmp default
+------------------+-----------+
| Field            | Value     |
+------------------+-----------+
| direction        | ingress   |
| ethertype        | IPv4      |
| protocol         | icmp      |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+

$ openstack security group rule create --ethertype IPv6 --proto ipv6-icmp default
+-----------+-----------+
| Field     | Value     |
+-----------+-----------+
| direction | ingress   |
| ethertype | IPv6      |
| protocol  | ipv6-icmp |
+-----------+-----------+

$ openstack security group rule create --proto tcp --dst-port 22 default
+------------------+-----------+
| Field            | Value     |
+------------------+-----------+
| direction        | ingress   |
| ethertype        | IPv4      |
| port_range_max   | 22        |
| port_range_min   | 22        |
| protocol         | tcp       |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+

$ openstack security group rule create --ethertype IPv6 --proto tcp --dst-port 22 default
+------------------+-----------+
| Field            | Value     |
+------------------+-----------+
| direction        | ingress   |
| ethertype        | IPv6      |
| port_range_max   | 22        |
| port_range_min   | 22        |
| protocol         | tcp       |
+------------------+-----------+
Launch an instance with an interface on the provider network. For example, a CirrOS image using flavor ID 1.

$ openstack server create --flavor 1 --image cirros \
  --nic net-id=NETWORK_ID provider-instance1
Replace NETWORK_ID with the ID of the provider network.

Determine the IPv4 and IPv6 addresses of the instance.

$ openstack server list
+--------------------------------------+--------------------+--------+------------------------------------------------------------+------------+
| ID                                   | Name               | Status | Networks                                                   | Image Name |
+--------------------------------------+--------------------+--------+------------------------------------------------------------+------------+
| 018e0ae2-b43c-4271-a78d-62653dd03285 | provider-instance1 | ACTIVE | provider1=203.0.113.13, fd00:203:0:113:f816:3eff:fe58:be4e | cirros     |
+--------------------------------------+--------------------+--------+------------------------------------------------------------+------------+
On the controller node or any host with access to the provider network, ping the IPv4 and IPv6 addresses of the instance.

$ ping -c 4 203.0.113.13
PING 203.0.113.13 (203.0.113.13) 56(84) bytes of data.
64 bytes from 203.0.113.13: icmp_req=1 ttl=63 time=3.18 ms
64 bytes from 203.0.113.13: icmp_req=2 ttl=63 time=0.981 ms
64 bytes from 203.0.113.13: icmp_req=3 ttl=63 time=1.06 ms
64 bytes from 203.0.113.13: icmp_req=4 ttl=63 time=0.929 ms

--- 203.0.113.13 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3002ms
rtt min/avg/max/mdev = 0.929/1.539/3.183/0.951 ms

$ ping6 -c 4 fd00:203:0:113:f816:3eff:fe58:be4e
PING fd00:203:0:113:f816:3eff:fe58:be4e(fd00:203:0:113:f816:3eff:fe58:be4e) 56 data bytes
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=1 ttl=64 time=1.25 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=2 ttl=64 time=0.683 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=3 ttl=64 time=0.762 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=4 ttl=64 time=0.486 ms

--- fd00:203:0:113:f816:3eff:fe58:be4e ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2999ms
rtt min/avg/max/mdev = 0.486/0.796/1.253/0.282 ms




Verify network operation¶
On each compute node, verify creation of the qdhcp namespace.

# ip netns
qdhcp-8b868082-e312-4110-8627-298109d4401c
Source a regular (non-administrative) project credentials.

Create the appropriate security group rules to allow ping and SSH access instances using the network.

$ openstack security group rule create --proto icmp default
+------------------+-----------+
| Field            | Value     |
+------------------+-----------+
| direction        | ingress   |
| ethertype        | IPv4      |
| protocol         | icmp      |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+

$ openstack security group rule create --ethertype IPv6 --proto ipv6-icmp default
+-----------+-----------+
| Field     | Value     |
+-----------+-----------+
| direction | ingress   |
| ethertype | IPv6      |
| protocol  | ipv6-icmp |
+-----------+-----------+

$ openstack security group rule create --proto tcp --dst-port 22 default
+------------------+-----------+
| Field            | Value     |
+------------------+-----------+
| direction        | ingress   |
| ethertype        | IPv4      |
| port_range_max   | 22        |
| port_range_min   | 22        |
| protocol         | tcp       |
| remote_ip_prefix | 0.0.0.0/0 |
+------------------+-----------+

$ openstack security group rule create --ethertype IPv6 --proto tcp --dst-port 22 default
+------------------+-----------+
| Field            | Value     |
+------------------+-----------+
| direction        | ingress   |
| ethertype        | IPv6      |
| port_range_max   | 22        |
| port_range_min   | 22        |
| protocol         | tcp       |
+------------------+-----------+
Launch an instance with an interface on the provider network. For example, a CirrOS image using flavor ID 1.

$ openstack server create --flavor 1 --image cirros \
  --nic net-id=NETWORK_ID provider-instance1
Replace NETWORK_ID with the ID of the provider network.

Determine the IPv4 and IPv6 addresses of the instance.

$ openstack server list
+--------------------------------------+--------------------+--------+------------------------------------------------------------+------------+
| ID                                   | Name               | Status | Networks                                                   | Image Name |
+--------------------------------------+--------------------+--------+------------------------------------------------------------+------------+
| 018e0ae2-b43c-4271-a78d-62653dd03285 | provider-instance1 | ACTIVE | provider1=203.0.113.13, fd00:203:0:113:f816:3eff:fe58:be4e | cirros     |
+--------------------------------------+--------------------+--------+------------------------------------------------------------+------------+
On the controller node or any host with access to the provider network, ping the IPv4 and IPv6 addresses of the instance.

$ ping -c 4 203.0.113.13
PING 203.0.113.13 (203.0.113.13) 56(84) bytes of data.
64 bytes from 203.0.113.13: icmp_req=1 ttl=63 time=3.18 ms
64 bytes from 203.0.113.13: icmp_req=2 ttl=63 time=0.981 ms
64 bytes from 203.0.113.13: icmp_req=3 ttl=63 time=1.06 ms
64 bytes from 203.0.113.13: icmp_req=4 ttl=63 time=0.929 ms

--- 203.0.113.13 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 3002ms
rtt min/avg/max/mdev = 0.929/1.539/3.183/0.951 ms

$ ping6 -c 4 fd00:203:0:113:f816:3eff:fe58:be4e
PING fd00:203:0:113:f816:3eff:fe58:be4e(fd00:203:0:113:f816:3eff:fe58:be4e) 56 data bytes
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=1 ttl=64 time=1.25 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=2 ttl=64 time=0.683 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=3 ttl=64 time=0.762 ms
64 bytes from fd00:203:0:113:f816:3eff:fe58:be4e icmp_seq=4 ttl=64 time=0.486 ms

--- fd00:203:0:113:f816:3eff:fe58:be4e ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2999ms
rtt min/avg/max/mdev = 0.486/0.796/1.253/0.282 ms
Obtain access to the instance.

Test IPv4 and IPv6 connectivity to the Internet or other external network.




openstack security group rule create --proto icmp default
openstack security group rule create --ethertype IPv6 --proto ipv6-icmp default
openstack security group rule create --proto tcp --dst-port 22 default
openstack security group rule create --ethertype IPv6 --proto tcp --dst-port 22 default




openstack network create  --share --provider-physical-network provider --provider-network-type flat public

openstack subnet create --network public \
 --allocation-pool start=192.168.56.100,end=192.168.56.200 \
 --dns-nameserver 192.168.56.1 --gateway 192.168.56.1 \
 --subnet-range 192.168.56.0/24 public-subnet


openstack security group rule create --proto icmp default

openstack security group rule create --proto tcp --dst-port 22 default




openstack network create --share --provider-physical-network provider --provider-network-type flat provider1

openstack subnet create --subnet-range 192.168.56.0/24 --gateway 192.168.56.1 \
  --network provider1 --allocation-pool start=192.168.56.120,end=192.168.56.200 \
  --dns-nameserver 8.8.4.4 provider1-v4

openstack subnet create --subnet-range 192.168.56.0/24 --gateway 192.168.56.1 \
  --network provider1 --dns-nameserver 8.8.4.4 provider1-v4


openstack security group rule create --proto icmp default


openstack server create --flavor m1.nano --image cirros --nic net-id=9d5443fa-0ae4-4270-8f00-67e6a68b299a cirros2

systemctl restart neutron-l3-agent.service
systemctl enable neutron-l3-agent.service

neutron agent-list


# Generated by NetworkManager
nameserver 223.6.6.6
nameserver 8.8.4.4
nameserver 8.8.8.8
nameserver 61.139.2.69



packstack --allinone --provision-demo=n --os-neutron-ml2-type-drivers=flat --os-neutron-ml2-mechanism-drivers=linuxbridge --os-neutron-ml2-flat-networks=physnet0 --os-neutron-l2-agent=linuxbridge --os-neutron-lb-interface-mappings=physnet0:eth1 --os-neutron-ml2-tenant-network-types=' ' --nagios-install=n 

source /root/keystonerc_admin


neutron net-create --shared --provider:network_type=flat --provider:physical_network=physnet0 othernet

neutron net-create --shared --provider:network_type=flat --provider:enp0s8 othernet

neutron net-create --shared --provider:network_type=flat --provider:enp0s8 othernet
neutron subnet-create --name other_subnet --enable_dhcp --allocation-pool=start=192.168.56.2,end=192.168.56.100 --gateway=192.168.56.1 --dns-nameserver=8.8.8.8 othernet 192.168.56.0/24






https://docs.openstack.org/mitaka/zh_CN/install-guide-rdo/neutron-controller-install-option1.html




cinder service-list



openstack 常用术语 
https://docs.openstack.org/ocata/zh_CN/install-guide-ubuntu/common/glossary.html








openstack 使用
知识点到考试的解读

一、Keystone认证模块基本出3个题

1、有关keystone的service和endpoint相关；

2、跟创建用户有关，包括domains,groups, projects, users,和roles；

3、跟用户的环境变量有关；

4、修改用户相关信息，包括密码等。

这个模块比较简单，要想通过一定不能丢分。

二、Glance镜像管理模块基本出3个题

1、上传镜像相关；

2、修改和添加镜像的相关参数；

3、镜像的导出。

镜像的后端这也要准备一下，比如考虑增加一个路径，这个模块比较简单，要想通过一定不能丢分。

三、块存储管理模块基本出3个题

1、跟据相关参数要求创建一个块存储；

2、创建一个加密卷；

3、卷的配额管理；

4、卷的快照管理；

5、会和计算结合挂载一个卷，卸载一个卷等。

结合知识点，这个模块出题点会多些，而且会结合计算模块一起考，要想通过一定不能丢分，特别是加密这个题，单词比较长不容易记忆要掌握好技巧。

四、对象储管理模块基本出3个题

1、上传指定对象文件；

2、结合用户或project对象文件的权限控制，包括读、写、查看；

3、下载指定对象文件到指定目录；

4、过期文件的管理。

按要求上传下载文件对象不要出错，一个题里会有多步操作的情况。

五、heat编排模块基本出2-3个题

1、根据要求创建stack；

2、查看stack的状态和输出信息；

3、根据要求更新stack。

目前不会要求你编辑heat模版，但练习时要练到，考虑到知识点有，以后考试可能会考。

六、Nova计算模块基本出5个题左右

1、根据要求创建一个实例；

2、修改实例的状态（关机、停止、恢复）；

3、创建并管理keypairs，能够用keypairs启动实例；

4、实例绑定和删除 Floating IP；

5、管理安全组规则；

6、管理实例快照，能够用快照启动实例；

7、管理配额。

7个知识会出5道左右的题，所以某些题是几个知识点的综合，分有多步操作。

七、Neutron网络模块基本出5个题左右

1、根据要求创建外部网络；

2、创建内部网络及子网；

3、创建路由、指定网关、并绑定接口；

4、管理project的安全组规则；

5、配额管理；

6、申请Floating IP,并绑定到实例，解绑Floating IP。

网络部分命令比较多相对复杂，网络部分最好有个网络拓扑结构图方便记忆相关命令。

八、Troubleshooting模块出4个题左右

1、要学会分析log信息，根据log信息排除相关问题；

2、学会查看各服务的状态；

3、要学会备份数据库、实例等信息；

4、能够排查网络故障。

这部分要求比较高，平时就要养成独立分析问题解决问题的能力。

九、界面操作

1、界面操作大概有一个题。

在考试时尽量不要用界面操作，除了两种情况：一是确认命令没有问题但没有出结果；二是考试时间不够了。说明一下考试时很多题是界面操作不了的，只能命令行操作。

十、OpenStack知识

1、OpenStack知识这部分不会出题。

能够使用API /CLI这部分就会给分的。


小结：考试共计32道操作题，考试时间2.5个小时，平均下来每个题不到5分钟，
再去除理解题意、拷贝等操作，实际的时间更少了，所以一定要熟练。


---------------------------------------------
display 随意
user name: admin
api key : 123456
auth service  http://Controller:5000/v3
keyston version ：3
use scope： domain  Default
Project name： admin
---------------------------------------------



[filter:keystoneauth]
use = egg:swift#keystoneauth
operator_roles = admin,user  ---- 在创建用户时候，用户必须是admin或者user角色。


什么是逻辑分区管理 LVM ，如何在Ubuntu中使用？

什么是 LVM?
!在LVM下，磁盘和分区可以抽象成一个含有多个磁盘和分区的设备。你的操作系统将不会知道这些区别，因为LVM只会给操作系统展示你设置的卷组（磁盘）和逻辑卷（分区）!!!!!!!!!!!!



逻辑分区管理是一个存在于磁盘/分区和操作系统之间的一个抽象层。在传统的磁盘管理中，你的操作系统寻找有哪些磁盘可用（/dev/sda、/dev/sdb等等），并且这些磁盘有哪些可用的分区（如/dev/sda1、/dev/sda2等等）。

在LVM下，磁盘和分区可以抽象成一个含有多个磁盘和分区的设备。你的操作系统将不会知道这些区别，因为LVM只会给操作系统展示你设置的卷组（磁盘）和逻辑卷（分区）

因为卷组和逻辑卷并不物理地对应到影片，因此可以很容易地动态调整和创建新的磁盘和分区。除此之外，LVM带来了你的文件系统所不具备的功能。比如，ext3不支持实时快照，但是如果你正在使用LVM你可以不卸载磁盘的情况下做一个逻辑卷的快照。

你什么时候该使用LVM？
一些发行版如Fedora已经默认安装了LVM。

如果你想要轻松地扩展(如调整分区大小等)或者想要将多块磁盘组成一个存储池，那么LVM或许正是你所寻找的。





pip install virtualenv virtualenvwrapper-win
mkvirtualenv openstack
git clone git://git.openstack.org/openstack/nova
workon openstack
pip install -r requirements.txt


pip install -r requirements.txt Ignoring Routes: markers "python_version!='2.7'" don't match your environment


牺牲一定程度的数据一致性来达到高可用性和可伸缩性，支持多租户模式、容器和对象读写操作，适合解决互联网的应用场景下非结构化数据存储问题。
一致性散列（Consistent Hashing)

面对海量级别的对象，需要存放在成千上万台服务器和硬盘设备上，首先要解决寻址问题，即如何将对象分布到这些设备地址上。Swift 是基于一致性散列技术，通过计算可将对象均匀分布到虚拟空间的虚拟节点上，在增加或删除节点时可大大减少需移动的数据量；虚拟空间大小通常采用 2 的 n 次幂，便于进行高效的移位操作；然后通过独特的数据结构 Ring（环）再将虚拟节点映射到实际的物理存储设备上，完成寻址过程。


数据一致性模型（Consistency Model）

按照 Eric Brewer 的 CAP（Consistency，Availability，Partition Tolerance）理论，无法同时满足 3 个方面，Swift 放弃严格一致性（满足 ACID 事务级别），而采用最终一致性模型（Eventual Consistency），来达到高可用性和无限水平扩展能力。
（1）定义：N：数据的副本总数；W：写操作被确认接受的副本数量；R：读操作的副本数量

（2）强一致性：R+W>N，以保证对副本的读写操作会产生交集，从而保证可以读取到最新版本；如果 W=N，R=1，则需要全部更新，适合大量读少量写操作场景下的强一致性；如果 R=N，W=1，则只更新一个副本，通过读取全部副本来得到最新版本，适合大量写少量读场景下的强一致性。

（3）弱一致性：R+W<=N，如果读写操作的副本集合不产生交集，就可能会读到脏数据；适合对一致性要求比较低的场景。




Windows hosts
C:\Windows\System32\drivers\etc
C:\Windows\System32\drivers\etc


openstack-service restart

for s in `systemctl list-unit-files | grep openstack| awk '{print $1}'`; do systemctl restart $s; done

for s in `systemctl list-unit-files | grep openstack| awk '{print $1}'`; do echo $s; done

for s in `systemctl list-unit-files | grep neutron| awk '{print $1}'`; do echo $s; done

for s in `systemctl list-unit-files | grep neutron| awk '{print $1}'`; do systemctl restart $s; done


#!/bin/bash

srv=$1
if [ $# -eq 2 ]
then
    action=$2
else
    action=restart
fi
echo usage action $action
cd /etc/init/; for i in $(ls $srv-* | cut -d \. -f 1 | xargs); do sudo service $i $action; done



重启openstack的整个服务
openstack-service restart


重启dashboard
service httpd  restart 
service memcached restart


重启 ceilometer
2.1 cinder
service mongod restart


2.2 controller
service openstack-ceilometer-api restart  
service openstack-ceilometer-notification restart
service openstack-ceilometer-central restart
service openstack-ceilometer-collector restart
service openstack-ceilometer-alarm-evaluator restart
service openstack-ceilometer-alarm-notifier restart

2.3 compute
service  openstack-nova-compute restart

2.4 controller
service  openstack-glance-api restart
service  openstack-glance-registry restart
Block Storage service

2.5 controller node
service   openstack-cinder-api restart
service   openstack-cinder-scheduler restart

2.6 cinder
service    openstack-cinder-volume  restart


重启Fuel服务

docker restart fuel-core-6.1-nailgun
docker restart fuel-core-6.1-keystone
docker restart fuel-core-6.1-rsync
docker restart fuel-core-6.1-mcollective
docker restart fuel-core-6.1-ostf
docker restart fuel-core-6.1-astute
docker restart fuel-core-6.1-rsyslog
docker restart fuel-core-6.1-postgres
docker restart fuel-core-6.1-rabbitmq
docker restart fuel-core-6.1-nginx
docker restart fuel-core-6.1-cobbler


重启 Neutron 服务
4.1 控制节点

service openstack-nova-api restart
service openstack-nova-scheduler restart
service openstack-nova-conductor restart
service neutron-server restart

4.2 网络节点
service openvswitch restart
service neutron-openvswitch-agent restart（fuel控制节点默认stop）
service neutron-l3-agent restart（fuel控制节点默认stop）
service neutron-dhcp-agent restart（fuel控制节点默认stop）
service neutron-metadata-agent restart（fuel控制节点默认stop）

4.3 计算节点
service neutron-openvswitch-agent restart
service openvswitch restart


重启cinder服务
5.1 控制节点

service openstack-cinder-api restart
service openstack-cinder-scheduler restart

5.2 存储节点
service openstack-cinder-volume restart


重启glance服务
6.1 控制节点

service openstack-glance-api restart
service openstack-glance-registry restart


重启Swift服务
7.1 控制节点

service openstack-swift-proxy restart
service memcached restart

7.2 存储节点
service openstack-swift-account restart
service openstack-swift-account-auditor restart
service openstack-swift-account-reaper restart
service openstack-swift-account-replicator restart
service openstack-swift-container restart
service openstack-swift-container-auditor restart
service openstack-swift-container-replicator restart
service openstack-swift-container-updater restart
service openstack-swift-object restart
service openstack-swift-object-auditor restart
service openstack-swift-object-replicator restart
service openstack-swift-object-updater restart


重启Nova服务
8.1 控制节点

service openstack-nova-api restart
service openstack-nova-cert restart
service openstack-nova-consoleauth restart
service openstack-nova-scheduler restart
service openstack-nova-conductor restart
service openstack-nova-novncproxy restart

8.2 计算节点
service libvirtd restart
service openstack-nova-compute restart


重启openstack的整个服务
openstack-service restart



1. 重启dashboard
service httpd  restart 
service memcached restart




2. 重启 ceilometer
2.1 cinder
service mongod restart
1
2.2 controller
service  openstack-ceilometer-api  restart    
service openstack-ceilometer-notification restart
service openstack-ceilometer-central  restart
service openstack-ceilometer-collector  restart
service openstack-ceilometer-alarm-evaluator  restart
service openstack-ceilometer-alarm-notifier restart

2.3 compute
service  openstack-nova-compute restart

2.4 controller
service  openstack-glance-api restart
service  openstack-glance-registry restart


Block Storage service

2.5 controller node
service   openstack-cinder-api restart
service   openstack-cinder-scheduler restart


2.6 cinder
service    openstack-cinder-volume  restart


3. 重启Fuel服务
docker restart  fuel-core-6.1-nailgun
docker restart fuel-core-6.1-keystone
docker restart fuel-core-6.1-rsync
docker restart fuel-core-6.1-mcollective
docker restart fuel-core-6.1-ostf
docker restart fuel-core-6.1-astute
docker restart fuel-core-6.1-rsyslog
docker restart fuel-core-6.1-postgres
docker restart fuel-core-6.1-rabbitmq
docker restart fuel-core-6.1-nginx
docker restart fuel-core-6.1-cobbler


4. 重启 Neutron 服务
4.1 控制节点
service openstack-nova-api restart
service openstack-nova-scheduler restart
service openstack-nova-conductor restart
service neutron-server restart


4.2 网络节点
service openvswitch restart
service neutron-openvswitch-agent restart（fuel控制节点默认stop）
service neutron-l3-agent restart（fuel控制节点默认stop）
service neutron-dhcp-agent restart（fuel控制节点默认stop）
service neutron-metadata-agent restart（fuel控制节点默认stop）

service openvswitch restart
service neutron-openvswitch-agent restart
service neutron-l3-agent restart
service neutron-dhcp-agent restart
service neutron-metadata-agent restart


4.3 计算节点
service neutron-openvswitch-agent restart
service openvswitch restart


5. 重启cinder服务
5.1 控制节点
service openstack-cinder-api restart
service openstack-cinder-scheduler restart

5.2 存储节点
service openstack-cinder-volume restart


6. 重启glance服务
6.1 控制节点
service openstack-glance-api restart
service openstack-glance-registry restart


7. 重启Swift服务
7.1 控制节点
service openstack-swift-proxy restart
service memcached restart

7.2 存储节点
service openstack-swift-account restart
service openstack-swift-account-auditor restart
service openstack-swift-account-reaper restart
service openstack-swift-account-replicator restart
service openstack-swift-container restart
service openstack-swift-container-auditor restart
service openstack-swift-container-replicator restart
service openstack-swift-container-updater restart
service openstack-swift-object restart
service openstack-swift-object-auditor restart
service openstack-swift-object-replicator restart
service openstack-swift-object-updater restart

8. 重启Nova服务
8.1 控制节点
service openstack-nova-api restart
service openstack-nova-cert restart
service openstack-nova-consoleauth restart
service openstack-nova-scheduler restart
service openstack-nova-conductor restart
service openstack-nova-novncproxy restart

8.2 计算节点
service libvirtd restart
service openstack-nova-compute restart



用户名:口令:用户标识号:组标识号:注释性描述:主目录:登录Shell

chattr
a：让文件或目录仅供附加用途。
b：不更新文件或目录的最后存取时间。
c：将文件或目录压缩后存放。
d：将文件或目录排除在倾倒操作之外。
i：不得任意更动文件或目录。
s：保密性删除文件或目录。
S：即时更新文件或目录。
u：预防意外删除。


让某个文件只能往里面追加数据，但不能删除，适用于各种日志文件：
# chattr +a /var/log/messages

1、用chattr命令防止系统中某个关键文件被修改：
# chattr +i /etc/resolv.conf



/sbin/sysctl -w net.ipv4.ip_forward=1

/etc/sysctl.conf
net.ipv4.ip_forward=1

sysctl -p

iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE
iptables -I FORWARD 1 -s 192.168.56.0/24 -j ACCEPT
iptables -I FORWARD 1 -d 192.168.56.0/24 -j ACCEPT




VNC不能访问的问题：
metadata_host=192.168.0.32
novncproxy_base_url=http://114.116.47.43:6080/vnc_auto.html
server_proxyclient_address=0.0.0.0
service  openstack-nova-compute restart

开启华为安全规则ALL:ALL


ip route add 192.168.1.0/24 dev eth0
net.ipv4.ip_forward = 1

/etc/sysctl.conf
net.ipv4.ip_forward = 1

sysctl -w net.ipv4.ip_forward=1

$ sudo iptables -A FORWARD -i ens38 -o ens39 -m state --state RELATED,ESTABLISHED -j ACCEPT
$ sudo iptables -A FORWARD -i ens39 -o ens38 -j ACCEPT


ip addr add 172.24.4.0/24 dev br-ex
iptables -A FORWARD -d 172.24.4.0/24 -j ACCEPT
iptables -A FORWARD -s 172.24.4.0/24 -j ACCEPT
iptables -t nat -I POSTROUTING 1 -s 172.24.4.0/24 -j MASQUERADE


ifconfig br-ex promisc
ifconfig eth0 promisc


ip route add 192.168.100.0/24 via 192.168.0.201


//------------------------------------------------
3、创建cont1容器：
openstack container create cont1
[root@controller ~]# openstack container create cont1
+---------------------------------------+-----------+------------------------------------+
| account                               | container | x-trans-id                         |
+---------------------------------------+-----------+------------------------------------+
| AUTH_506f9094723b4860b01ad275c87d83d6 | cont1     | tx053c325d7dc440d6bc1e3-005c651100 |
+---------------------------------------+-----------+------------------------------------+

4、将测试文件上传到container1容器：
openstack object create cont1 testfile1

[root@controller ~]# openstack object create cont1 testfile1
+-----------+-----------+----------------------------------+
| object    | container | etag                             |
+-----------+-----------+----------------------------------+
| testfile1 | cont1     | b61fea1c05fcc0d6bfb43d020274dea6 |
+-----------+-----------+----------------------------------+

5、在container1容器中列出文件：
openstack object list cont1
[root@controller ~]# openstack object list cont1
+-----------+
| Name      |
+-----------+
| testfile1 |
+-----------+

6、从container1容器中下载一个测试文件：
openstack object save cont1 testfile2

删除本地testfile1文件
[root@controller ~]# rm testfile1 

从容器下载testfile1文件至本地
[root@controller ~]# openstack object save cont1 testfile1

查看testfile1
[root@controller ~]# more testfile1 


openstack server create --flavor mm1.tiny --image cirros --nic net-id=27109cd0-1fee-4931-98ae-c1e35cbe0ed7 vm2


ubuntu 使用aliyun 源

cp sources.list sources.list.bak

# deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted
deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe
deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse
deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties
deb http://archive.canonical.com/ubuntu xenial partner
deb-src http://archive.canonical.com/ubuntu xenial partner
deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted
deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties
deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe
deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse


linux bridge 配置端口转发
Chain FORWARD (policy ACCEPT)
target     prot opt source               destination         
ACCEPT     all  --  anywhere             192.168.122.0/24     ctstate RELATED,ESTABLISHED
ACCEPT     all  --  192.168.122.0/24     anywhere            
ACCEPT     all  --  anywhere             anywhere            
REJECT     all  --  anywhere             anywhere             reject-with icmp-port-unreachable
REJECT     all  --  anywhere             anywhere             reject-with icmp-port-unreachable


root@lsdir-VirtualBox:/etc/network# more interfaces
# interfaces(5) file used by ifup(8) and ifdown(8)
auto lo
iface lo inet loopback

auto enp0s8
iface enp0s8 inet static
address 192.168.56.126
netmask 255.255.255.0
gateway 192.168.56.1
dns-nameserver 223.6.6.6 8.8.8.8


root@lsdir-VirtualBox:~# route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.56.1    0.0.0.0         UG    0      0        0 enp0s8
0.0.0.0         10.0.2.2        0.0.0.0         UG    100    0        0 enp0s3
10.0.2.0        0.0.0.0         255.255.255.0   U     100    0        0 enp0s3
169.254.0.0     0.0.0.0         255.255.0.0     U     1000   0        0 enp0s8
192.168.56.0    0.0.0.0         255.255.255.0   U     0      0        0 enp0s8
192.168.56.0    0.0.0.0         255.255.255.0   U     100    0        0 enp0s9
192.168.122.0   0.0.0.0         255.255.255.0   U     0      0        0 virbr0


service apparmor stop
update-rc.d -f apparmor remove 

brctl delif br0 vnet0
brctl addif virbr0 vnet0

#root@kvm-host:~# brctl addbr br0        #增加一个虚拟网桥br0

#root@kvm-host:~# brctl addif br0 eth0    #在br0中添加一个接口eth0

#root@kvm-host:~# brctl stp br0 on        #打开STP协议，否则可能造成环路

#root@kvm-host:~# ifconfig eth0 0        #将eth0的IP设置为0

#root@kvm-host:~# dhclient br0          #设置动态给br0配置ip、route等

root@bridge:~> brctl addbr br0
其次，我们不需要STP(生成树协议)等。因为我们只有一个路由器，是绝对不可能形成一个环的。我们可以关闭这个功能。（这样也可以减少网络环境的数据包污染）：


iptables -A FORWARD -d 192.168.99.0/24 -j ACCEPT
iptables -A FORWARD -s 192.168.99.0/24 -j ACCEPT
iptables -t nat -I POSTROUTING 1 -s 192.168.99.0/24 -j MASQUERADE


网络
brctl是Linux下用来管理以太网桥，在内核中建立、维护、检查网桥配置的命令
STP － Spanning Tree Protocol（生成树协议）逻辑上断开环路，防止二层网络的广播风暴的产生
以dhcp模式启用 'eth0'

 
在计算机网络中，TUN与TAP是操作系统内核中的虚拟网络设备。不同于普通靠硬件网路板卡实现的设备，这些虚拟的网络设备全部用软件实现，并向运行于操作系统上的软件提供与硬件的网络设备完全相同的功能。
TAP等同于一个以太网设备，它操作第二层数据包如以太网数据帧。TUN模拟了网络层设备，操作第三层数据包比如IP数据封包。
 
#1.创建kvm桥接网络模式，要安装bridge-utils tunctl
yum install bridge-utils tunctl
 
添加一个br0网桥(桥接类型)
brctl addbr br0
ifconfig br0 up
 

#分步执行网络会断开
------------------------
将br0与eth0绑定在一起
brctl addif br0 eth0
将br0设置为启用STP协议
brctl stp br0 on
将eth0的IP设置为0
ifconfig eth0 0
使用dhcp为br0分配IP
dhclient br0
-------------------------
 
#最佳方式（注意修改成自己的IP）
brctl addif br0 eth0 && brctl stp br0 on && ifconfig eth0 0.0.0.0 && ifconfig br0 192.168.52.201 netmask 255.255.255.0 && route add default gw 192.168.52.1
 
#创建TAP类型虚拟网卡设备
tunctl -b -t vnet0
ifconfig vnet0 up
brctl addif br0 vnet0
brctl show
 
#创建虚拟机并关联网卡
/usr/libexec/qemu-kvm -m 4096 -smp 1 -boot order=cd -hda /cloud/Centos.img -net nic -net tap,ifname=vnet0,script=no,downscript=no
#创建虚拟机并关联网卡并添加mac地址
/usr/libexec/qemu-kvm -m 2048 -smp 1 -boot order=cd -hda /cloud/Centos.img -net nic,macaddr=52:54:00:12:34:57 -net tap,ifname=vnet0,script=no,downscript=no
 
#将磁盘设置成半虚拟化virtio
<disk type="file" device="disk">
<driver name="qemu" type="qcow2" />
<source file="/cloud/centos.img" />
<target dev='vda' bus='virtio'/>
</disk>
 
 
 
libvirt
libvirt是一套免费、开源的支持Linux下主流虚拟化工具的C函数库，其旨在为包括Xen在内的各种虚拟化工具提供一套方便、可靠的编程接口，支持与C,C++,Ruby,Python,JAVA等多种主流开发语言的绑定。当前主流Linux平台上默认的虚拟化管理工具virt-manager(图形化),virt-install（命令行模式）等均基于libvirt开发而成。
Libvirt库是一种实现 Linux 虚拟化功能的 Linux API，它支持各种虚拟机监控程序，包括 Xen 和 KVM，以及 QEMU 和用于其他操作系统的一些虚拟产品
 
#安装libvirt
yum install libvirt
#启动libvirt
service libvirtd start
#启动后会多一个virbr0网桥，该网桥是NAT类型
 
virsh(非常好的虚拟化命令行管理工具，两种模式：交换模式和非交换模式)
定义虚拟机
virsh define /cloud/centos-base.xml
 
virsh 进行管理虚拟机
virsh# list --all # 显示所有虚拟机 --all显示全部
 
启动虚拟机
#virsh start centos
 
关闭虚拟机
#virsh shutdown centos
 
强制关机
#virsh destroy centos
 
移除虚拟机
#virsh undefine centos
 
显示vnc端口
#virsh vncdisplay centos
 
动态查询kvm使用资源
#top -d 1 | grep kvm
 
查询kvm进程
ps -aux | grep kvm
 
开机自动启动虚拟机
#virsh autostart centos
 
导出虚拟机centos的硬件配置信息为/cloud/centos.bak.xml
#virsh dumpxml centos > /cloud/centos.bak.xml
 
编辑虚拟机配置
#virsh edit centos


ubuntu 下重启网络
一般以为是用下面这条 
sudo service network restart
但其实在ubuntu里要用 network-manager
sudo service network-manager restart


iptables -A FORWARD -d 192.168.56.0/24 -j ACCEPT
iptables -A FORWARD -s 192.168.56.0/24 -j ACCEPT
iptables -t nat -I POSTROUTING 1 -s 192.168.56.0/24 -j MASQUERADE



添加本地源
wget -m -c -np -nH --cut-dirs=4 -e robots=off http://vault.centos.org/7.2.1511/cloud/x86_64/openstack-newton/ -P
wget -m -c -np -nH --cut-dirs=3 -e robots=off http://mirrors.aliyun.com/ceph/rpm-hammer/el7/ -P /root/ceph

wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.163.com/repo/Centos-7.repo

[root@controller yum.repos.d]# more openstack.repo
[openstack-rocky]
name=openstack-rocky
baseurl=file:///root/openstack-rocky
enabled=1
gpgcheck=0



keepcache setting to 1 in the /etc/yum.conf file as follows:

# vi /etc/yum.conf
keepcache = 1

# yum clean packages

# yum clean metadata

To delete package headers, use the following command:

# yum clean headers
To clean all cached information, use the following command:

# yum clean all
If you get the message “Metadata file does not match checksum” during a Yum operation, clearing the metadata from the cache might not help. In this case, adding the following line to /etc/yum.conf resolves the problem:

# vi /etc/yum.conf
http_caching=none


